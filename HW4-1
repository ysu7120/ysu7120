{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW4-1","provenance":[],"collapsed_sections":[],"mount_file_id":"14nQCCxIjyFFcJXsdFr9esl18eUaBpmUR","authorship_tag":"ABX9TyO0zVIlwjXpDrdE4Qc/3U5i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rcyjF_UfHhud","executionInfo":{"status":"ok","timestamp":1620045056208,"user_tz":-540,"elapsed":264993,"user":{"displayName":"유성운","photoUrl":"","userId":"04010736311021015655"}},"outputId":"125cf1cf-0fbd-4c5a-cd7b-95b8e74e9a90"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import svm\n","import pandas as pd\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.neural_network import MLPClassifier\n","from google.colab import drive\n","\n","#csv파일을 저장\n","drive.mount('/content/drive')\n","csv_path = '/content/drive/MyDrive/propublica_data_for_fairml.csv' #구글 드라이브에 csv파일을 마운트\n","df = pd.read_csv(csv_path)\n","\n","#이진분류를 위해 y를 타겟변수로 잡는다\n","x = df[['Number_of_Priors','score_factor','Age_Above_FourtyFive','Age_Below_TwentyFive','African_American','Asian','Hispanic','Native_American','Other','Female','Misdemeanor']]\n","y = df['Two_yr_Recidivism']\n","\n","#훈련사이즈를 0.6으로 지정\n","x_train, x_test, y_train, y_test = train_test_split(x,y,train_size=0.6)\n","\n","#MLPClassifier를 활용하여 전처리과정을 실시\n","mlp = MLPClassifier(hidden_layer_sizes=(100),\n","                    learning_rate_init=0.001,\n","                    batch_size=32,\n","                    solver='sgd',\n","                    verbose=True)\n","mlp.fit(x_train, y_train)\n","\n","res = mlp.predict(x_test)\n","\n","#k-폴드 교차검증을 실시\n","accuracies = cross_val_score(mlp, x_train, y_train, cv=10) #10폴드를 위해 cv=10으로 지정\n","print(\"평균정확률:\",accuracies.mean()*100,\"%\") # 평균정확률(accuracy)을 구한다, 결과: 약 68.9%\n","\n","#MLPClassifier를 활용하여 전처리과정을 실시\n","mlp = MLPClassifier(hidden_layer_sizes=(100,100),#은닉층을 2개로 증가\n","                    learning_rate_init=0.001,\n","                    batch_size=32,\n","                    solver='sgd',\n","                    verbose=True)\n","mlp.fit(x_train, y_train)\n","\n","res = mlp.predict(x_test)\n","\n","accuracies2 = cross_val_score(mlp, x_train, y_train, cv=10)\n","print(\"평균정확률:\",accuracies2.mean()*100,\"%\")# 평균정확률(accuracy)을 구한다, 결과: 약 68.8%\n","\n","#MLPClassifier를 활용하여 전처리과정을 실시\n","mlp = MLPClassifier(hidden_layer_sizes=(100,100,100),#은닉층을 3개로 증가\n","                    learning_rate_init=0.001,\n","                    batch_size=32,\n","                    solver='sgd',\n","                    verbose=True)\n","mlp.fit(x_train, y_train)\n","\n","res = mlp.predict(x_test)\n","\n","accuracies3 = cross_val_score(mlp, x_train, y_train, cv=10)\n","print(\"평균정확률:\",accuracies3.mean()*100,\"%\")# 평균정확률(accuracy)을 구한다, 결과: 약 68.5%"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Iteration 1, loss = 0.67623737\n","Iteration 2, loss = 0.64540454\n","Iteration 3, loss = 0.63348011\n","Iteration 4, loss = 0.62546873\n","Iteration 5, loss = 0.61939239\n","Iteration 6, loss = 0.61388825\n","Iteration 7, loss = 0.60979836\n","Iteration 8, loss = 0.60581686\n","Iteration 9, loss = 0.60380951\n","Iteration 10, loss = 0.60121179\n","Iteration 11, loss = 0.59995449\n","Iteration 12, loss = 0.59912514\n","Iteration 13, loss = 0.59712419\n","Iteration 14, loss = 0.59772809\n","Iteration 15, loss = 0.59632306\n","Iteration 16, loss = 0.59596658\n","Iteration 17, loss = 0.59462533\n","Iteration 18, loss = 0.59438650\n","Iteration 19, loss = 0.59404934\n","Iteration 20, loss = 0.59345553\n","Iteration 21, loss = 0.59289659\n","Iteration 22, loss = 0.59254350\n","Iteration 23, loss = 0.59233581\n","Iteration 24, loss = 0.59246190\n","Iteration 25, loss = 0.59234314\n","Iteration 26, loss = 0.59228665\n","Iteration 27, loss = 0.59156681\n","Iteration 28, loss = 0.59178332\n","Iteration 29, loss = 0.59123110\n","Iteration 30, loss = 0.59123593\n","Iteration 31, loss = 0.59095035\n","Iteration 32, loss = 0.59183992\n","Iteration 33, loss = 0.59169289\n","Iteration 34, loss = 0.59094420\n","Iteration 35, loss = 0.59076444\n","Iteration 36, loss = 0.59060805\n","Iteration 37, loss = 0.59123507\n","Iteration 38, loss = 0.59052453\n","Iteration 39, loss = 0.59074629\n","Iteration 40, loss = 0.59026187\n","Iteration 41, loss = 0.58999670\n","Iteration 42, loss = 0.58951680\n","Iteration 43, loss = 0.58982752\n","Iteration 44, loss = 0.58947606\n","Iteration 45, loss = 0.58907030\n","Iteration 46, loss = 0.58977718\n","Iteration 47, loss = 0.58982452\n","Iteration 48, loss = 0.58945696\n","Iteration 49, loss = 0.58972441\n","Iteration 50, loss = 0.58975024\n","Iteration 51, loss = 0.59011783\n","Iteration 52, loss = 0.58918000\n","Iteration 53, loss = 0.58879316\n","Iteration 54, loss = 0.58911594\n","Iteration 55, loss = 0.58943440\n","Iteration 56, loss = 0.58847654\n","Iteration 57, loss = 0.58957705\n","Iteration 58, loss = 0.58870420\n","Iteration 59, loss = 0.58885147\n","Iteration 60, loss = 0.58868815\n","Iteration 61, loss = 0.58826100\n","Iteration 62, loss = 0.58819933\n","Iteration 63, loss = 0.58892132\n","Iteration 64, loss = 0.58818947\n","Iteration 65, loss = 0.58840966\n","Iteration 66, loss = 0.58811893\n","Iteration 67, loss = 0.58676455\n","Iteration 68, loss = 0.58799986\n","Iteration 69, loss = 0.58745300\n","Iteration 70, loss = 0.58697821\n","Iteration 71, loss = 0.58771499\n","Iteration 72, loss = 0.58824081\n","Iteration 73, loss = 0.58717494\n","Iteration 74, loss = 0.58878593\n","Iteration 75, loss = 0.58724353\n","Iteration 76, loss = 0.58659776\n","Iteration 77, loss = 0.58762356\n","Iteration 78, loss = 0.58750822\n","Iteration 79, loss = 0.58751131\n","Iteration 80, loss = 0.58683525\n","Iteration 81, loss = 0.58695576\n","Iteration 82, loss = 0.58631488\n","Iteration 83, loss = 0.58694085\n","Iteration 84, loss = 0.58690091\n","Iteration 85, loss = 0.58735861\n","Iteration 86, loss = 0.58674561\n","Iteration 87, loss = 0.58672028\n","Iteration 88, loss = 0.58713218\n","Iteration 89, loss = 0.58690382\n","Iteration 90, loss = 0.58615392\n","Iteration 91, loss = 0.58716748\n","Iteration 92, loss = 0.58592970\n","Iteration 93, loss = 0.58662699\n","Iteration 94, loss = 0.58679058\n","Iteration 95, loss = 0.58689372\n","Iteration 96, loss = 0.58753787\n","Iteration 97, loss = 0.58676268\n","Iteration 98, loss = 0.58653474\n","Iteration 99, loss = 0.58794717\n","Iteration 100, loss = 0.58687753\n","Iteration 101, loss = 0.58589860\n","Iteration 102, loss = 0.58667740\n","Iteration 103, loss = 0.58659510\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.69097370\n","Iteration 2, loss = 0.65205174\n","Iteration 3, loss = 0.63393256\n","Iteration 4, loss = 0.62431102\n","Iteration 5, loss = 0.61966263\n","Iteration 6, loss = 0.61154775\n","Iteration 7, loss = 0.60675754\n","Iteration 8, loss = 0.60286685\n","Iteration 9, loss = 0.60079430\n","Iteration 10, loss = 0.59805005\n","Iteration 11, loss = 0.59819634\n","Iteration 12, loss = 0.59493016\n","Iteration 13, loss = 0.59551064\n","Iteration 14, loss = 0.59345917\n","Iteration 15, loss = 0.59250940\n","Iteration 16, loss = 0.59163080\n","Iteration 17, loss = 0.59118769\n","Iteration 18, loss = 0.59083796\n","Iteration 19, loss = 0.59079547\n","Iteration 20, loss = 0.59033515\n","Iteration 21, loss = 0.58941222\n","Iteration 22, loss = 0.58979590\n","Iteration 23, loss = 0.58852741\n","Iteration 24, loss = 0.59129189\n","Iteration 25, loss = 0.58883543\n","Iteration 26, loss = 0.58806921\n","Iteration 27, loss = 0.58909299\n","Iteration 28, loss = 0.58748954\n","Iteration 29, loss = 0.58984163\n","Iteration 30, loss = 0.58731350\n","Iteration 31, loss = 0.58782281\n","Iteration 32, loss = 0.58750563\n","Iteration 33, loss = 0.58702449\n","Iteration 34, loss = 0.58692201\n","Iteration 35, loss = 0.58726565\n","Iteration 36, loss = 0.58736796\n","Iteration 37, loss = 0.58588003\n","Iteration 38, loss = 0.58737910\n","Iteration 39, loss = 0.58694183\n","Iteration 40, loss = 0.58657685\n","Iteration 41, loss = 0.58584879\n","Iteration 42, loss = 0.58646449\n","Iteration 43, loss = 0.58701046\n","Iteration 44, loss = 0.58849463\n","Iteration 45, loss = 0.58727908\n","Iteration 46, loss = 0.58642490\n","Iteration 47, loss = 0.58611055\n","Iteration 48, loss = 0.58616713\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.67888602\n","Iteration 2, loss = 0.64132466\n","Iteration 3, loss = 0.62639408\n","Iteration 4, loss = 0.61569966\n","Iteration 5, loss = 0.61045084\n","Iteration 6, loss = 0.60608348\n","Iteration 7, loss = 0.60242676\n","Iteration 8, loss = 0.60074309\n","Iteration 9, loss = 0.59789622\n","Iteration 10, loss = 0.59719547\n","Iteration 11, loss = 0.59654985\n","Iteration 12, loss = 0.59465763\n","Iteration 13, loss = 0.59346126\n","Iteration 14, loss = 0.59319314\n","Iteration 15, loss = 0.59268287\n","Iteration 16, loss = 0.59193318\n","Iteration 17, loss = 0.59232865\n","Iteration 18, loss = 0.59116205\n","Iteration 19, loss = 0.59085128\n","Iteration 20, loss = 0.59047136\n","Iteration 21, loss = 0.59100733\n","Iteration 22, loss = 0.59010779\n","Iteration 23, loss = 0.59015229\n","Iteration 24, loss = 0.59092584\n","Iteration 25, loss = 0.58965234\n","Iteration 26, loss = 0.58920347\n","Iteration 27, loss = 0.59049848\n","Iteration 28, loss = 0.58893426\n","Iteration 29, loss = 0.58915041\n","Iteration 30, loss = 0.58850798\n","Iteration 31, loss = 0.58859493\n","Iteration 32, loss = 0.58857331\n","Iteration 33, loss = 0.58967383\n","Iteration 34, loss = 0.58899717\n","Iteration 35, loss = 0.58792243\n","Iteration 36, loss = 0.58937023\n","Iteration 37, loss = 0.58974218\n","Iteration 38, loss = 0.58740983\n","Iteration 39, loss = 0.58826754\n","Iteration 40, loss = 0.58746187\n","Iteration 41, loss = 0.58788097\n","Iteration 42, loss = 0.58734708\n","Iteration 43, loss = 0.58799765\n","Iteration 44, loss = 0.58786450\n","Iteration 45, loss = 0.58696794\n","Iteration 46, loss = 0.58813981\n","Iteration 47, loss = 0.58759933\n","Iteration 48, loss = 0.58712105\n","Iteration 49, loss = 0.58609872\n","Iteration 50, loss = 0.58688576\n","Iteration 51, loss = 0.58730124\n","Iteration 52, loss = 0.58669089\n","Iteration 53, loss = 0.58599648\n","Iteration 54, loss = 0.58919326\n","Iteration 55, loss = 0.58665286\n","Iteration 56, loss = 0.58749710\n","Iteration 57, loss = 0.58658179\n","Iteration 58, loss = 0.58646035\n","Iteration 59, loss = 0.58718370\n","Iteration 60, loss = 0.58529273\n","Iteration 61, loss = 0.58685109\n","Iteration 62, loss = 0.58603791\n","Iteration 63, loss = 0.58649101\n","Iteration 64, loss = 0.58794978\n","Iteration 65, loss = 0.58596059\n","Iteration 66, loss = 0.58608099\n","Iteration 67, loss = 0.58602490\n","Iteration 68, loss = 0.58624986\n","Iteration 69, loss = 0.58670553\n","Iteration 70, loss = 0.58626865\n","Iteration 71, loss = 0.58623961\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.65579807\n","Iteration 2, loss = 0.63046265\n","Iteration 3, loss = 0.62116089\n","Iteration 4, loss = 0.61441298\n","Iteration 5, loss = 0.60977322\n","Iteration 6, loss = 0.60666034\n","Iteration 7, loss = 0.60333346\n","Iteration 8, loss = 0.60087785\n","Iteration 9, loss = 0.59989163\n","Iteration 10, loss = 0.59869754\n","Iteration 11, loss = 0.59725336\n","Iteration 12, loss = 0.59523271\n","Iteration 13, loss = 0.59429656\n","Iteration 14, loss = 0.59293820\n","Iteration 15, loss = 0.59328646\n","Iteration 16, loss = 0.59209481\n","Iteration 17, loss = 0.59183657\n","Iteration 18, loss = 0.59249933\n","Iteration 19, loss = 0.59189424\n","Iteration 20, loss = 0.59119058\n","Iteration 21, loss = 0.59059081\n","Iteration 22, loss = 0.59098910\n","Iteration 23, loss = 0.59002691\n","Iteration 24, loss = 0.59036102\n","Iteration 25, loss = 0.59074176\n","Iteration 26, loss = 0.59054207\n","Iteration 27, loss = 0.59038522\n","Iteration 28, loss = 0.58927409\n","Iteration 29, loss = 0.58998463\n","Iteration 30, loss = 0.58933705\n","Iteration 31, loss = 0.59021267\n","Iteration 32, loss = 0.58913940\n","Iteration 33, loss = 0.58853117\n","Iteration 34, loss = 0.58865242\n","Iteration 35, loss = 0.59000846\n","Iteration 36, loss = 0.58928915\n","Iteration 37, loss = 0.58867749\n","Iteration 38, loss = 0.58821382\n","Iteration 39, loss = 0.58924938\n","Iteration 40, loss = 0.58836698\n","Iteration 41, loss = 0.58885397\n","Iteration 42, loss = 0.58891048\n","Iteration 43, loss = 0.58810480\n","Iteration 44, loss = 0.58830220\n","Iteration 45, loss = 0.58898670\n","Iteration 46, loss = 0.58810648\n","Iteration 47, loss = 0.58794346\n","Iteration 48, loss = 0.58897221\n","Iteration 49, loss = 0.58755727\n","Iteration 50, loss = 0.58744794\n","Iteration 51, loss = 0.58835869\n","Iteration 52, loss = 0.58878119\n","Iteration 53, loss = 0.58829818\n","Iteration 54, loss = 0.58770114\n","Iteration 55, loss = 0.58611814\n","Iteration 56, loss = 0.58869216\n","Iteration 57, loss = 0.58763078\n","Iteration 58, loss = 0.58880230\n","Iteration 59, loss = 0.58785931\n","Iteration 60, loss = 0.58721157\n","Iteration 61, loss = 0.58719127\n","Iteration 62, loss = 0.58695339\n","Iteration 63, loss = 0.58727921\n","Iteration 64, loss = 0.58685023\n","Iteration 65, loss = 0.58826315\n","Iteration 66, loss = 0.58856559\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.66442439\n","Iteration 2, loss = 0.64021891\n","Iteration 3, loss = 0.62738870\n","Iteration 4, loss = 0.61963083\n","Iteration 5, loss = 0.61416600\n","Iteration 6, loss = 0.60979098\n","Iteration 7, loss = 0.60607847\n","Iteration 8, loss = 0.60410570\n","Iteration 9, loss = 0.60240566\n","Iteration 10, loss = 0.60069818\n","Iteration 11, loss = 0.59870980\n","Iteration 12, loss = 0.59885093\n","Iteration 13, loss = 0.59731909\n","Iteration 14, loss = 0.59650489\n","Iteration 15, loss = 0.59662354\n","Iteration 16, loss = 0.59448854\n","Iteration 17, loss = 0.59463461\n","Iteration 18, loss = 0.59420578\n","Iteration 19, loss = 0.59428094\n","Iteration 20, loss = 0.59396846\n","Iteration 21, loss = 0.59401841\n","Iteration 22, loss = 0.59390651\n","Iteration 23, loss = 0.59336805\n","Iteration 24, loss = 0.59283056\n","Iteration 25, loss = 0.59312080\n","Iteration 26, loss = 0.59264337\n","Iteration 27, loss = 0.59284057\n","Iteration 28, loss = 0.59231667\n","Iteration 29, loss = 0.59291148\n","Iteration 30, loss = 0.59136043\n","Iteration 31, loss = 0.59154226\n","Iteration 32, loss = 0.59211926\n","Iteration 33, loss = 0.59193550\n","Iteration 34, loss = 0.59176027\n","Iteration 35, loss = 0.59192150\n","Iteration 36, loss = 0.59189696\n","Iteration 37, loss = 0.59094542\n","Iteration 38, loss = 0.59126630\n","Iteration 39, loss = 0.59142190\n","Iteration 40, loss = 0.59164138\n","Iteration 41, loss = 0.59169584\n","Iteration 42, loss = 0.59293711\n","Iteration 43, loss = 0.59149075\n","Iteration 44, loss = 0.59038852\n","Iteration 45, loss = 0.59098358\n","Iteration 46, loss = 0.59086587\n","Iteration 47, loss = 0.59116524\n","Iteration 48, loss = 0.59086233\n","Iteration 49, loss = 0.59047364\n","Iteration 50, loss = 0.59074750\n","Iteration 51, loss = 0.59067059\n","Iteration 52, loss = 0.59003175\n","Iteration 53, loss = 0.59184390\n","Iteration 54, loss = 0.59021222\n","Iteration 55, loss = 0.59076401\n","Iteration 56, loss = 0.58978055\n","Iteration 57, loss = 0.59033353\n","Iteration 58, loss = 0.59022294\n","Iteration 59, loss = 0.59012559\n","Iteration 60, loss = 0.59010446\n","Iteration 61, loss = 0.59046556\n","Iteration 62, loss = 0.58944272\n","Iteration 63, loss = 0.59069331\n","Iteration 64, loss = 0.58990525\n","Iteration 65, loss = 0.58965942\n","Iteration 66, loss = 0.58954441\n","Iteration 67, loss = 0.59019453\n","Iteration 68, loss = 0.58979821\n","Iteration 69, loss = 0.59003729\n","Iteration 70, loss = 0.58934363\n","Iteration 71, loss = 0.58831907\n","Iteration 72, loss = 0.58966526\n","Iteration 73, loss = 0.58929870\n","Iteration 74, loss = 0.59032726\n","Iteration 75, loss = 0.59006067\n","Iteration 76, loss = 0.58931825\n","Iteration 77, loss = 0.58926627\n","Iteration 78, loss = 0.58924222\n","Iteration 79, loss = 0.58929246\n","Iteration 80, loss = 0.58838648\n","Iteration 81, loss = 0.58794266\n","Iteration 82, loss = 0.58888700\n","Iteration 83, loss = 0.58981262\n","Iteration 84, loss = 0.58883908\n","Iteration 85, loss = 0.58850364\n","Iteration 86, loss = 0.58889786\n","Iteration 87, loss = 0.58846305\n","Iteration 88, loss = 0.58941840\n","Iteration 89, loss = 0.58883335\n","Iteration 90, loss = 0.58865210\n","Iteration 91, loss = 0.58875149\n","Iteration 92, loss = 0.58862334\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.68868758\n","Iteration 2, loss = 0.65219460\n","Iteration 3, loss = 0.63760386\n","Iteration 4, loss = 0.62691292\n","Iteration 5, loss = 0.62038905\n","Iteration 6, loss = 0.61411803\n","Iteration 7, loss = 0.61035362\n","Iteration 8, loss = 0.60673364\n","Iteration 9, loss = 0.60503285\n","Iteration 10, loss = 0.60201496\n","Iteration 11, loss = 0.60000557\n","Iteration 12, loss = 0.59816158\n","Iteration 13, loss = 0.59770934\n","Iteration 14, loss = 0.59662016\n","Iteration 15, loss = 0.59667094\n","Iteration 16, loss = 0.59594115\n","Iteration 17, loss = 0.59472271\n","Iteration 18, loss = 0.59211012\n","Iteration 19, loss = 0.59588896\n","Iteration 20, loss = 0.59450935\n","Iteration 21, loss = 0.59432592\n","Iteration 22, loss = 0.59434477\n","Iteration 23, loss = 0.59417208\n","Iteration 24, loss = 0.59404584\n","Iteration 25, loss = 0.59325350\n","Iteration 26, loss = 0.59272250\n","Iteration 27, loss = 0.59315019\n","Iteration 28, loss = 0.59276563\n","Iteration 29, loss = 0.59293310\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.68690596\n","Iteration 2, loss = 0.65507106\n","Iteration 3, loss = 0.63756793\n","Iteration 4, loss = 0.63162873\n","Iteration 5, loss = 0.62303938\n","Iteration 6, loss = 0.61839233\n","Iteration 7, loss = 0.61408817\n","Iteration 8, loss = 0.61077243\n","Iteration 9, loss = 0.60869395\n","Iteration 10, loss = 0.60591262\n","Iteration 11, loss = 0.60286216\n","Iteration 12, loss = 0.60191183\n","Iteration 13, loss = 0.60038110\n","Iteration 14, loss = 0.59868429\n","Iteration 15, loss = 0.59887509\n","Iteration 16, loss = 0.59677008\n","Iteration 17, loss = 0.59700685\n","Iteration 18, loss = 0.59558427\n","Iteration 19, loss = 0.59572997\n","Iteration 20, loss = 0.59528838\n","Iteration 21, loss = 0.59359192\n","Iteration 22, loss = 0.59320408\n","Iteration 23, loss = 0.59304174\n","Iteration 24, loss = 0.59281763\n","Iteration 25, loss = 0.59286622\n","Iteration 26, loss = 0.59456316\n","Iteration 27, loss = 0.59165191\n","Iteration 28, loss = 0.59234226\n","Iteration 29, loss = 0.59212581\n","Iteration 30, loss = 0.59140826\n","Iteration 31, loss = 0.59173720\n","Iteration 32, loss = 0.59221172\n","Iteration 33, loss = 0.59130208\n","Iteration 34, loss = 0.59134850\n","Iteration 35, loss = 0.59220569\n","Iteration 36, loss = 0.59112114\n","Iteration 37, loss = 0.59108212\n","Iteration 38, loss = 0.59073363\n","Iteration 39, loss = 0.59111138\n","Iteration 40, loss = 0.58918682\n","Iteration 41, loss = 0.59088822\n","Iteration 42, loss = 0.59072769\n","Iteration 43, loss = 0.59034858\n","Iteration 44, loss = 0.59101061\n","Iteration 45, loss = 0.58992481\n","Iteration 46, loss = 0.59028847\n","Iteration 47, loss = 0.58925062\n","Iteration 48, loss = 0.59066159\n","Iteration 49, loss = 0.58983595\n","Iteration 50, loss = 0.59090380\n","Iteration 51, loss = 0.58859927\n","Iteration 52, loss = 0.58910283\n","Iteration 53, loss = 0.59053610\n","Iteration 54, loss = 0.58943559\n","Iteration 55, loss = 0.58982704\n","Iteration 56, loss = 0.58900100\n","Iteration 57, loss = 0.58882864\n","Iteration 58, loss = 0.58882917\n","Iteration 59, loss = 0.58805606\n","Iteration 60, loss = 0.58902113\n","Iteration 61, loss = 0.58943911\n","Iteration 62, loss = 0.58854717\n","Iteration 63, loss = 0.58862191\n","Iteration 64, loss = 0.58930534\n","Iteration 65, loss = 0.58930994\n","Iteration 66, loss = 0.58946108\n","Iteration 67, loss = 0.58883904\n","Iteration 68, loss = 0.58921841\n","Iteration 69, loss = 0.58826207\n","Iteration 70, loss = 0.58839016\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.66253097\n","Iteration 2, loss = 0.64016900\n","Iteration 3, loss = 0.62915075\n","Iteration 4, loss = 0.62312440\n","Iteration 5, loss = 0.61818034\n","Iteration 6, loss = 0.61384004\n","Iteration 7, loss = 0.61111841\n","Iteration 8, loss = 0.60831125\n","Iteration 9, loss = 0.60603871\n","Iteration 10, loss = 0.60356031\n","Iteration 11, loss = 0.60351140\n","Iteration 12, loss = 0.60230142\n","Iteration 13, loss = 0.60098721\n","Iteration 14, loss = 0.60026270\n","Iteration 15, loss = 0.59997203\n","Iteration 16, loss = 0.59834576\n","Iteration 17, loss = 0.59803909\n","Iteration 18, loss = 0.59810588\n","Iteration 19, loss = 0.59841167\n","Iteration 20, loss = 0.59772325\n","Iteration 21, loss = 0.59644033\n","Iteration 22, loss = 0.59636579\n","Iteration 23, loss = 0.59605518\n","Iteration 24, loss = 0.59602190\n","Iteration 25, loss = 0.59599840\n","Iteration 26, loss = 0.59549927\n","Iteration 27, loss = 0.59467196\n","Iteration 28, loss = 0.59548461\n","Iteration 29, loss = 0.59544735\n","Iteration 30, loss = 0.59489895\n","Iteration 31, loss = 0.59401295\n","Iteration 32, loss = 0.59486862\n","Iteration 33, loss = 0.59520085\n","Iteration 34, loss = 0.59446286\n","Iteration 35, loss = 0.59451847\n","Iteration 36, loss = 0.59427287\n","Iteration 37, loss = 0.59352624\n","Iteration 38, loss = 0.59637972\n","Iteration 39, loss = 0.59371274\n","Iteration 40, loss = 0.59405751\n","Iteration 41, loss = 0.59364174\n","Iteration 42, loss = 0.59356930\n","Iteration 43, loss = 0.59260406\n","Iteration 44, loss = 0.59475619\n","Iteration 45, loss = 0.59371375\n","Iteration 46, loss = 0.59361103\n","Iteration 47, loss = 0.59286514\n","Iteration 48, loss = 0.59332800\n","Iteration 49, loss = 0.59389308\n","Iteration 50, loss = 0.59312513\n","Iteration 51, loss = 0.59269701\n","Iteration 52, loss = 0.59282084\n","Iteration 53, loss = 0.59245294\n","Iteration 54, loss = 0.59239240\n","Iteration 55, loss = 0.59218999\n","Iteration 56, loss = 0.59284563\n","Iteration 57, loss = 0.59253375\n","Iteration 58, loss = 0.59153514\n","Iteration 59, loss = 0.59217347\n","Iteration 60, loss = 0.59225347\n","Iteration 61, loss = 0.59302411\n","Iteration 62, loss = 0.59164859\n","Iteration 63, loss = 0.59223753\n","Iteration 64, loss = 0.59262036\n","Iteration 65, loss = 0.59158921\n","Iteration 66, loss = 0.59193594\n","Iteration 67, loss = 0.59239605\n","Iteration 68, loss = 0.59119591\n","Iteration 69, loss = 0.59271925\n","Iteration 70, loss = 0.59233337\n","Iteration 71, loss = 0.59223873\n","Iteration 72, loss = 0.59123899\n","Iteration 73, loss = 0.59182565\n","Iteration 74, loss = 0.59233615\n","Iteration 75, loss = 0.59297368\n","Iteration 76, loss = 0.59085079\n","Iteration 77, loss = 0.59241081\n","Iteration 78, loss = 0.59140748\n","Iteration 79, loss = 0.59114993\n","Iteration 80, loss = 0.59208289\n","Iteration 81, loss = 0.59149474\n","Iteration 82, loss = 0.59062786\n","Iteration 83, loss = 0.59157446\n","Iteration 84, loss = 0.59085087\n","Iteration 85, loss = 0.59094946\n","Iteration 86, loss = 0.59116588\n","Iteration 87, loss = 0.59151849\n","Iteration 88, loss = 0.59154745\n","Iteration 89, loss = 0.59104080\n","Iteration 90, loss = 0.59027114\n","Iteration 91, loss = 0.59100209\n","Iteration 92, loss = 0.59103072\n","Iteration 93, loss = 0.59064746\n","Iteration 94, loss = 0.59090490\n","Iteration 95, loss = 0.59033981\n","Iteration 96, loss = 0.59039446\n","Iteration 97, loss = 0.59029436\n","Iteration 98, loss = 0.59066628\n","Iteration 99, loss = 0.59077487\n","Iteration 100, loss = 0.59039862\n","Iteration 101, loss = 0.59073893\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.67852717\n","Iteration 2, loss = 0.63397119\n","Iteration 3, loss = 0.61804983\n","Iteration 4, loss = 0.61017496\n","Iteration 5, loss = 0.60631907\n","Iteration 6, loss = 0.60330400\n","Iteration 7, loss = 0.60140741\n","Iteration 8, loss = 0.59904177\n","Iteration 9, loss = 0.59779874\n","Iteration 10, loss = 0.59648486\n","Iteration 11, loss = 0.59550931\n","Iteration 12, loss = 0.59558171\n","Iteration 13, loss = 0.59335689\n","Iteration 14, loss = 0.59246220\n","Iteration 15, loss = 0.59302000\n","Iteration 16, loss = 0.59323914\n","Iteration 17, loss = 0.59216398\n","Iteration 18, loss = 0.59156253\n","Iteration 19, loss = 0.59085204\n","Iteration 20, loss = 0.59085946\n","Iteration 21, loss = 0.59020168\n","Iteration 22, loss = 0.59000441\n","Iteration 23, loss = 0.58945135\n","Iteration 24, loss = 0.58950101\n","Iteration 25, loss = 0.59032283\n","Iteration 26, loss = 0.58906846\n","Iteration 27, loss = 0.58977401\n","Iteration 28, loss = 0.58812808\n","Iteration 29, loss = 0.58909918\n","Iteration 30, loss = 0.58986857\n","Iteration 31, loss = 0.58868298\n","Iteration 32, loss = 0.58939148\n","Iteration 33, loss = 0.58845347\n","Iteration 34, loss = 0.58830070\n","Iteration 35, loss = 0.58811736\n","Iteration 36, loss = 0.58846405\n","Iteration 37, loss = 0.58795152\n","Iteration 38, loss = 0.58793548\n","Iteration 39, loss = 0.58784203\n","Iteration 40, loss = 0.58804726\n","Iteration 41, loss = 0.58821327\n","Iteration 42, loss = 0.58800396\n","Iteration 43, loss = 0.58685579\n","Iteration 44, loss = 0.58755622\n","Iteration 45, loss = 0.58783008\n","Iteration 46, loss = 0.58716633\n","Iteration 47, loss = 0.58754254\n","Iteration 48, loss = 0.58754002\n","Iteration 49, loss = 0.58696720\n","Iteration 50, loss = 0.58760631\n","Iteration 51, loss = 0.58811154\n","Iteration 52, loss = 0.58744512\n","Iteration 53, loss = 0.58676386\n","Iteration 54, loss = 0.58618024\n","Iteration 55, loss = 0.58736192\n","Iteration 56, loss = 0.58668275\n","Iteration 57, loss = 0.58697859\n","Iteration 58, loss = 0.58653547\n","Iteration 59, loss = 0.58713295\n","Iteration 60, loss = 0.58709033\n","Iteration 61, loss = 0.58689609\n","Iteration 62, loss = 0.58595376\n","Iteration 63, loss = 0.58700209\n","Iteration 64, loss = 0.58661001\n","Iteration 65, loss = 0.58533410\n","Iteration 66, loss = 0.58672588\n","Iteration 67, loss = 0.58652406\n","Iteration 68, loss = 0.58711873\n","Iteration 69, loss = 0.58625591\n","Iteration 70, loss = 0.58639888\n","Iteration 71, loss = 0.58630500\n","Iteration 72, loss = 0.58795659\n","Iteration 73, loss = 0.58639862\n","Iteration 74, loss = 0.58671258\n","Iteration 75, loss = 0.58655388\n","Iteration 76, loss = 0.58563052\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.64306419\n","Iteration 2, loss = 0.63168221\n","Iteration 3, loss = 0.62339330\n","Iteration 4, loss = 0.61639798\n","Iteration 5, loss = 0.61195166\n","Iteration 6, loss = 0.60854825\n","Iteration 7, loss = 0.60491222\n","Iteration 8, loss = 0.60198604\n","Iteration 9, loss = 0.59992604\n","Iteration 10, loss = 0.59790132\n","Iteration 11, loss = 0.59679089\n","Iteration 12, loss = 0.59509689\n","Iteration 13, loss = 0.59494405\n","Iteration 14, loss = 0.59361674\n","Iteration 15, loss = 0.59276840\n","Iteration 16, loss = 0.59267913\n","Iteration 17, loss = 0.59118325\n","Iteration 18, loss = 0.59007696\n","Iteration 19, loss = 0.59076293\n","Iteration 20, loss = 0.58969101\n","Iteration 21, loss = 0.58835629\n","Iteration 22, loss = 0.58958078\n","Iteration 23, loss = 0.58771712\n","Iteration 24, loss = 0.58821222\n","Iteration 25, loss = 0.58842534\n","Iteration 26, loss = 0.58712791\n","Iteration 27, loss = 0.58707928\n","Iteration 28, loss = 0.58674189\n","Iteration 29, loss = 0.58724807\n","Iteration 30, loss = 0.58699358\n","Iteration 31, loss = 0.58681904\n","Iteration 32, loss = 0.58660276\n","Iteration 33, loss = 0.58669468\n","Iteration 34, loss = 0.58581079\n","Iteration 35, loss = 0.58644241\n","Iteration 36, loss = 0.58650903\n","Iteration 37, loss = 0.58521044\n","Iteration 38, loss = 0.58671160\n","Iteration 39, loss = 0.58545293\n","Iteration 40, loss = 0.58494878\n","Iteration 41, loss = 0.58537487\n","Iteration 42, loss = 0.58561241\n","Iteration 43, loss = 0.58508041\n","Iteration 44, loss = 0.58432585\n","Iteration 45, loss = 0.58428639\n","Iteration 46, loss = 0.58467099\n","Iteration 47, loss = 0.58486800\n","Iteration 48, loss = 0.58368100\n","Iteration 49, loss = 0.58448024\n","Iteration 50, loss = 0.58354002\n","Iteration 51, loss = 0.58422571\n","Iteration 52, loss = 0.58400693\n","Iteration 53, loss = 0.58478764\n","Iteration 54, loss = 0.58501127\n","Iteration 55, loss = 0.58453814\n","Iteration 56, loss = 0.58393176\n","Iteration 57, loss = 0.58667141\n","Iteration 58, loss = 0.58390029\n","Iteration 59, loss = 0.58380966\n","Iteration 60, loss = 0.58515113\n","Iteration 61, loss = 0.58340588\n","Iteration 62, loss = 0.58339569\n","Iteration 63, loss = 0.58342419\n","Iteration 64, loss = 0.58348847\n","Iteration 65, loss = 0.58294793\n","Iteration 66, loss = 0.58347921\n","Iteration 67, loss = 0.58365365\n","Iteration 68, loss = 0.58491910\n","Iteration 69, loss = 0.58303135\n","Iteration 70, loss = 0.58333353\n","Iteration 71, loss = 0.58352646\n","Iteration 72, loss = 0.58368504\n","Iteration 73, loss = 0.58339605\n","Iteration 74, loss = 0.58297755\n","Iteration 75, loss = 0.58455941\n","Iteration 76, loss = 0.58262084\n","Iteration 77, loss = 0.58296275\n","Iteration 78, loss = 0.58282734\n","Iteration 79, loss = 0.58433110\n","Iteration 80, loss = 0.58392767\n","Iteration 81, loss = 0.58274995\n","Iteration 82, loss = 0.58344990\n","Iteration 83, loss = 0.58245839\n","Iteration 84, loss = 0.58288339\n","Iteration 85, loss = 0.58298359\n","Iteration 86, loss = 0.58219431\n","Iteration 87, loss = 0.58295348\n","Iteration 88, loss = 0.58194088\n","Iteration 89, loss = 0.58259596\n","Iteration 90, loss = 0.58217562\n","Iteration 91, loss = 0.58250720\n","Iteration 92, loss = 0.58228144\n","Iteration 93, loss = 0.58312868\n","Iteration 94, loss = 0.58265970\n","Iteration 95, loss = 0.58301755\n","Iteration 96, loss = 0.58253683\n","Iteration 97, loss = 0.58204468\n","Iteration 98, loss = 0.58244080\n","Iteration 99, loss = 0.58219750\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.68075811\n","Iteration 2, loss = 0.65268036\n","Iteration 3, loss = 0.63698029\n","Iteration 4, loss = 0.62675759\n","Iteration 5, loss = 0.62032293\n","Iteration 6, loss = 0.61559058\n","Iteration 7, loss = 0.61048791\n","Iteration 8, loss = 0.60809330\n","Iteration 9, loss = 0.60546491\n","Iteration 10, loss = 0.60441843\n","Iteration 11, loss = 0.60162296\n","Iteration 12, loss = 0.59993143\n","Iteration 13, loss = 0.59952057\n","Iteration 14, loss = 0.59889311\n","Iteration 15, loss = 0.59801986\n","Iteration 16, loss = 0.59803589\n","Iteration 17, loss = 0.59690069\n","Iteration 18, loss = 0.59696678\n","Iteration 19, loss = 0.59682086\n","Iteration 20, loss = 0.59560518\n","Iteration 21, loss = 0.59548161\n","Iteration 22, loss = 0.59490360\n","Iteration 23, loss = 0.59478319\n","Iteration 24, loss = 0.59477684\n","Iteration 25, loss = 0.59619763\n","Iteration 26, loss = 0.59438652\n","Iteration 27, loss = 0.59451904\n","Iteration 28, loss = 0.59471510\n","Iteration 29, loss = 0.59349602\n","Iteration 30, loss = 0.59506003\n","Iteration 31, loss = 0.59355824\n","Iteration 32, loss = 0.59500799\n","Iteration 33, loss = 0.59371531\n","Iteration 34, loss = 0.59373886\n","Iteration 35, loss = 0.59287820\n","Iteration 36, loss = 0.59355986\n","Iteration 37, loss = 0.59318300\n","Iteration 38, loss = 0.59279598\n","Iteration 39, loss = 0.59325849\n","Iteration 40, loss = 0.59220895\n","Iteration 41, loss = 0.59304487\n","Iteration 42, loss = 0.59296554\n","Iteration 43, loss = 0.59287510\n","Iteration 44, loss = 0.59274085\n","Iteration 45, loss = 0.59372062\n","Iteration 46, loss = 0.59221059\n","Iteration 47, loss = 0.59352078\n","Iteration 48, loss = 0.59239903\n","Iteration 49, loss = 0.59188285\n","Iteration 50, loss = 0.59291656\n","Iteration 51, loss = 0.59240398\n","Iteration 52, loss = 0.59231055\n","Iteration 53, loss = 0.59243912\n","Iteration 54, loss = 0.59233880\n","Iteration 55, loss = 0.59298379\n","Iteration 56, loss = 0.59141867\n","Iteration 57, loss = 0.59208851\n","Iteration 58, loss = 0.59196861\n","Iteration 59, loss = 0.59189673\n","Iteration 60, loss = 0.59254625\n","Iteration 61, loss = 0.59118045\n","Iteration 62, loss = 0.59123790\n","Iteration 63, loss = 0.59151763\n","Iteration 64, loss = 0.59142232\n","Iteration 65, loss = 0.59125844\n","Iteration 66, loss = 0.59281477\n","Iteration 67, loss = 0.59143859\n","Iteration 68, loss = 0.59093981\n","Iteration 69, loss = 0.59134243\n","Iteration 70, loss = 0.59122842\n","Iteration 71, loss = 0.59084800\n","Iteration 72, loss = 0.59212616\n","Iteration 73, loss = 0.59091659\n","Iteration 74, loss = 0.59124690\n","Iteration 75, loss = 0.59115710\n","Iteration 76, loss = 0.59306279\n","Iteration 77, loss = 0.59276836\n","Iteration 78, loss = 0.59126800\n","Iteration 79, loss = 0.59076331\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","평균정확률: 68.91789903110659 %\n","Iteration 1, loss = 0.67116251\n","Iteration 2, loss = 0.64137028\n","Iteration 3, loss = 0.62732357\n","Iteration 4, loss = 0.61966784\n","Iteration 5, loss = 0.61348571\n","Iteration 6, loss = 0.60854021\n","Iteration 7, loss = 0.60469412\n","Iteration 8, loss = 0.60267690\n","Iteration 9, loss = 0.59909536\n","Iteration 10, loss = 0.59768323\n","Iteration 11, loss = 0.59569330\n","Iteration 12, loss = 0.59580707\n","Iteration 13, loss = 0.59391132\n","Iteration 14, loss = 0.59239718\n","Iteration 15, loss = 0.59365865\n","Iteration 16, loss = 0.59182239\n","Iteration 17, loss = 0.59117172\n","Iteration 18, loss = 0.59020773\n","Iteration 19, loss = 0.59091609\n","Iteration 20, loss = 0.59056710\n","Iteration 21, loss = 0.58999716\n","Iteration 22, loss = 0.58988895\n","Iteration 23, loss = 0.59040334\n","Iteration 24, loss = 0.58964975\n","Iteration 25, loss = 0.59059867\n","Iteration 26, loss = 0.58864914\n","Iteration 27, loss = 0.58830491\n","Iteration 28, loss = 0.58907249\n","Iteration 29, loss = 0.58860872\n","Iteration 30, loss = 0.58830892\n","Iteration 31, loss = 0.58972628\n","Iteration 32, loss = 0.59012462\n","Iteration 33, loss = 0.58690792\n","Iteration 34, loss = 0.58790743\n","Iteration 35, loss = 0.58884155\n","Iteration 36, loss = 0.58921294\n","Iteration 37, loss = 0.58828267\n","Iteration 38, loss = 0.58781553\n","Iteration 39, loss = 0.58841912\n","Iteration 40, loss = 0.58771355\n","Iteration 41, loss = 0.58876746\n","Iteration 42, loss = 0.58715534\n","Iteration 43, loss = 0.58721858\n","Iteration 44, loss = 0.58717085\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.67170535\n","Iteration 2, loss = 0.63426143\n","Iteration 3, loss = 0.61966570\n","Iteration 4, loss = 0.61237478\n","Iteration 5, loss = 0.60755128\n","Iteration 6, loss = 0.60502056\n","Iteration 7, loss = 0.59946810\n","Iteration 8, loss = 0.59808997\n","Iteration 9, loss = 0.59547378\n","Iteration 10, loss = 0.59409079\n","Iteration 11, loss = 0.59189586\n","Iteration 12, loss = 0.59204173\n","Iteration 13, loss = 0.59136326\n","Iteration 14, loss = 0.58990190\n","Iteration 15, loss = 0.58871381\n","Iteration 16, loss = 0.58813122\n","Iteration 17, loss = 0.58738717\n","Iteration 18, loss = 0.58769357\n","Iteration 19, loss = 0.58809072\n","Iteration 20, loss = 0.58641540\n","Iteration 21, loss = 0.58653516\n","Iteration 22, loss = 0.58711488\n","Iteration 23, loss = 0.58704289\n","Iteration 24, loss = 0.58795970\n","Iteration 25, loss = 0.58722765\n","Iteration 26, loss = 0.58532022\n","Iteration 27, loss = 0.58559787\n","Iteration 28, loss = 0.58631460\n","Iteration 29, loss = 0.58648308\n","Iteration 30, loss = 0.58593627\n","Iteration 31, loss = 0.58493257\n","Iteration 32, loss = 0.58532094\n","Iteration 33, loss = 0.58515740\n","Iteration 34, loss = 0.58614520\n","Iteration 35, loss = 0.58445174\n","Iteration 36, loss = 0.58441565\n","Iteration 37, loss = 0.58527909\n","Iteration 38, loss = 0.58546826\n","Iteration 39, loss = 0.58470059\n","Iteration 40, loss = 0.58310906\n","Iteration 41, loss = 0.58392754\n","Iteration 42, loss = 0.58457718\n","Iteration 43, loss = 0.58409428\n","Iteration 44, loss = 0.58430194\n","Iteration 45, loss = 0.58399390\n","Iteration 46, loss = 0.58410955\n","Iteration 47, loss = 0.58410767\n","Iteration 48, loss = 0.58427075\n","Iteration 49, loss = 0.58391471\n","Iteration 50, loss = 0.58383684\n","Iteration 51, loss = 0.58237779\n","Iteration 52, loss = 0.58360314\n","Iteration 53, loss = 0.58263831\n","Iteration 54, loss = 0.58325805\n","Iteration 55, loss = 0.58457358\n","Iteration 56, loss = 0.58312655\n","Iteration 57, loss = 0.58267731\n","Iteration 58, loss = 0.58327363\n","Iteration 59, loss = 0.58287909\n","Iteration 60, loss = 0.58388355\n","Iteration 61, loss = 0.58298987\n","Iteration 62, loss = 0.58325274\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.65226002\n","Iteration 2, loss = 0.62495829\n","Iteration 3, loss = 0.61465672\n","Iteration 4, loss = 0.60759744\n","Iteration 5, loss = 0.60361087\n","Iteration 6, loss = 0.60031918\n","Iteration 7, loss = 0.59845849\n","Iteration 8, loss = 0.59477974\n","Iteration 9, loss = 0.59376321\n","Iteration 10, loss = 0.59231538\n","Iteration 11, loss = 0.59151892\n","Iteration 12, loss = 0.59200435\n","Iteration 13, loss = 0.59127865\n","Iteration 14, loss = 0.59190000\n","Iteration 15, loss = 0.58951944\n","Iteration 16, loss = 0.58956894\n","Iteration 17, loss = 0.59028372\n","Iteration 18, loss = 0.58845334\n","Iteration 19, loss = 0.59096847\n","Iteration 20, loss = 0.58920658\n","Iteration 21, loss = 0.58804067\n","Iteration 22, loss = 0.58751765\n","Iteration 23, loss = 0.58672061\n","Iteration 24, loss = 0.58815772\n","Iteration 25, loss = 0.58829067\n","Iteration 26, loss = 0.58873466\n","Iteration 27, loss = 0.58696214\n","Iteration 28, loss = 0.58782226\n","Iteration 29, loss = 0.58754726\n","Iteration 30, loss = 0.58739290\n","Iteration 31, loss = 0.58690347\n","Iteration 32, loss = 0.58587017\n","Iteration 33, loss = 0.58660547\n","Iteration 34, loss = 0.58802545\n","Iteration 35, loss = 0.58682978\n","Iteration 36, loss = 0.58596910\n","Iteration 37, loss = 0.58571176\n","Iteration 38, loss = 0.58680303\n","Iteration 39, loss = 0.58600879\n","Iteration 40, loss = 0.58601846\n","Iteration 41, loss = 0.58600960\n","Iteration 42, loss = 0.58579183\n","Iteration 43, loss = 0.58627901\n","Iteration 44, loss = 0.58630857\n","Iteration 45, loss = 0.58606596\n","Iteration 46, loss = 0.58638854\n","Iteration 47, loss = 0.58520866\n","Iteration 48, loss = 0.58491666\n","Iteration 49, loss = 0.58642569\n","Iteration 50, loss = 0.58524493\n","Iteration 51, loss = 0.58603952\n","Iteration 52, loss = 0.58525862\n","Iteration 53, loss = 0.58481508\n","Iteration 54, loss = 0.58505501\n","Iteration 55, loss = 0.58497277\n","Iteration 56, loss = 0.58472325\n","Iteration 57, loss = 0.58460465\n","Iteration 58, loss = 0.58534487\n","Iteration 59, loss = 0.58501058\n","Iteration 60, loss = 0.58422775\n","Iteration 61, loss = 0.58373051\n","Iteration 62, loss = 0.58556688\n","Iteration 63, loss = 0.58441458\n","Iteration 64, loss = 0.58349526\n","Iteration 65, loss = 0.58362432\n","Iteration 66, loss = 0.58427595\n","Iteration 67, loss = 0.58268467\n","Iteration 68, loss = 0.58428010\n","Iteration 69, loss = 0.58369806\n","Iteration 70, loss = 0.58385755\n","Iteration 71, loss = 0.58308941\n","Iteration 72, loss = 0.58313365\n","Iteration 73, loss = 0.58291473\n","Iteration 74, loss = 0.58348319\n","Iteration 75, loss = 0.58366388\n","Iteration 76, loss = 0.58291976\n","Iteration 77, loss = 0.58301399\n","Iteration 78, loss = 0.58237438\n","Iteration 79, loss = 0.58345397\n","Iteration 80, loss = 0.58290643\n","Iteration 81, loss = 0.58294020\n","Iteration 82, loss = 0.58379271\n","Iteration 83, loss = 0.58358480\n","Iteration 84, loss = 0.58359065\n","Iteration 85, loss = 0.58417242\n","Iteration 86, loss = 0.58275606\n","Iteration 87, loss = 0.58309323\n","Iteration 88, loss = 0.58416842\n","Iteration 89, loss = 0.58146892\n","Iteration 90, loss = 0.58345668\n","Iteration 91, loss = 0.58431063\n","Iteration 92, loss = 0.58394460\n","Iteration 93, loss = 0.58305405\n","Iteration 94, loss = 0.58230527\n","Iteration 95, loss = 0.58238898\n","Iteration 96, loss = 0.58261170\n","Iteration 97, loss = 0.58210977\n","Iteration 98, loss = 0.58348325\n","Iteration 99, loss = 0.58210130\n","Iteration 100, loss = 0.58200573\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.65848797\n","Iteration 2, loss = 0.63629835\n","Iteration 3, loss = 0.62545515\n","Iteration 4, loss = 0.61813598\n","Iteration 5, loss = 0.61295328\n","Iteration 6, loss = 0.60830154\n","Iteration 7, loss = 0.60420589\n","Iteration 8, loss = 0.60149008\n","Iteration 9, loss = 0.59911244\n","Iteration 10, loss = 0.59621742\n","Iteration 11, loss = 0.59490613\n","Iteration 12, loss = 0.59448496\n","Iteration 13, loss = 0.59307553\n","Iteration 14, loss = 0.59182666\n","Iteration 15, loss = 0.59124948\n","Iteration 16, loss = 0.58947579\n","Iteration 17, loss = 0.58991947\n","Iteration 18, loss = 0.59010821\n","Iteration 19, loss = 0.59031932\n","Iteration 20, loss = 0.58928498\n","Iteration 21, loss = 0.58894324\n","Iteration 22, loss = 0.58878899\n","Iteration 23, loss = 0.58911342\n","Iteration 24, loss = 0.58875231\n","Iteration 25, loss = 0.58753736\n","Iteration 26, loss = 0.58772414\n","Iteration 27, loss = 0.58872237\n","Iteration 28, loss = 0.58781941\n","Iteration 29, loss = 0.58752464\n","Iteration 30, loss = 0.58715075\n","Iteration 31, loss = 0.58768502\n","Iteration 32, loss = 0.58870455\n","Iteration 33, loss = 0.58760844\n","Iteration 34, loss = 0.58690170\n","Iteration 35, loss = 0.58921810\n","Iteration 36, loss = 0.58730367\n","Iteration 37, loss = 0.58772235\n","Iteration 38, loss = 0.58705128\n","Iteration 39, loss = 0.58741647\n","Iteration 40, loss = 0.58793353\n","Iteration 41, loss = 0.58754990\n","Iteration 42, loss = 0.58687642\n","Iteration 43, loss = 0.58574264\n","Iteration 44, loss = 0.58726217\n","Iteration 45, loss = 0.58572162\n","Iteration 46, loss = 0.58712308\n","Iteration 47, loss = 0.58656803\n","Iteration 48, loss = 0.58577079\n","Iteration 49, loss = 0.58683317\n","Iteration 50, loss = 0.58499693\n","Iteration 51, loss = 0.58752717\n","Iteration 52, loss = 0.58650725\n","Iteration 53, loss = 0.58769242\n","Iteration 54, loss = 0.58601010\n","Iteration 55, loss = 0.58653586\n","Iteration 56, loss = 0.58548274\n","Iteration 57, loss = 0.58622715\n","Iteration 58, loss = 0.58652705\n","Iteration 59, loss = 0.58594363\n","Iteration 60, loss = 0.58492196\n","Iteration 61, loss = 0.58614664\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.68696905\n","Iteration 2, loss = 0.65110276\n","Iteration 3, loss = 0.63593995\n","Iteration 4, loss = 0.62762691\n","Iteration 5, loss = 0.62058236\n","Iteration 6, loss = 0.61472679\n","Iteration 7, loss = 0.60915101\n","Iteration 8, loss = 0.60586878\n","Iteration 9, loss = 0.60405561\n","Iteration 10, loss = 0.59991090\n","Iteration 11, loss = 0.59778878\n","Iteration 12, loss = 0.59621792\n","Iteration 13, loss = 0.59611803\n","Iteration 14, loss = 0.59344754\n","Iteration 15, loss = 0.59383288\n","Iteration 16, loss = 0.59438385\n","Iteration 17, loss = 0.59286185\n","Iteration 18, loss = 0.59120997\n","Iteration 19, loss = 0.59137399\n","Iteration 20, loss = 0.59125600\n","Iteration 21, loss = 0.59152786\n","Iteration 22, loss = 0.59210138\n","Iteration 23, loss = 0.59159636\n","Iteration 24, loss = 0.59013452\n","Iteration 25, loss = 0.59114980\n","Iteration 26, loss = 0.59038090\n","Iteration 27, loss = 0.59000987\n","Iteration 28, loss = 0.58963716\n","Iteration 29, loss = 0.59064874\n","Iteration 30, loss = 0.58995490\n","Iteration 31, loss = 0.59117340\n","Iteration 32, loss = 0.58931876\n","Iteration 33, loss = 0.59001963\n","Iteration 34, loss = 0.58902420\n","Iteration 35, loss = 0.58936840\n","Iteration 36, loss = 0.58906281\n","Iteration 37, loss = 0.58930103\n","Iteration 38, loss = 0.58978347\n","Iteration 39, loss = 0.58834653\n","Iteration 40, loss = 0.58874186\n","Iteration 41, loss = 0.58823078\n","Iteration 42, loss = 0.58837450\n","Iteration 43, loss = 0.58943796\n","Iteration 44, loss = 0.58872361\n","Iteration 45, loss = 0.58866889\n","Iteration 46, loss = 0.58831134\n","Iteration 47, loss = 0.58736612\n","Iteration 48, loss = 0.58817161\n","Iteration 49, loss = 0.58810979\n","Iteration 50, loss = 0.58813260\n","Iteration 51, loss = 0.58765171\n","Iteration 52, loss = 0.58801223\n","Iteration 53, loss = 0.58678681\n","Iteration 54, loss = 0.58752720\n","Iteration 55, loss = 0.58664281\n","Iteration 56, loss = 0.58749767\n","Iteration 57, loss = 0.58645204\n","Iteration 58, loss = 0.58795451\n","Iteration 59, loss = 0.58835262\n","Iteration 60, loss = 0.58706173\n","Iteration 61, loss = 0.58628543\n","Iteration 62, loss = 0.58627542\n","Iteration 63, loss = 0.58652341\n","Iteration 64, loss = 0.58799075\n","Iteration 65, loss = 0.58736134\n","Iteration 66, loss = 0.58729465\n","Iteration 67, loss = 0.58686627\n","Iteration 68, loss = 0.58654325\n","Iteration 69, loss = 0.58634745\n","Iteration 70, loss = 0.58776026\n","Iteration 71, loss = 0.58768448\n","Iteration 72, loss = 0.58607610\n","Iteration 73, loss = 0.58546933\n","Iteration 74, loss = 0.58701095\n","Iteration 75, loss = 0.58659706\n","Iteration 76, loss = 0.58495005\n","Iteration 77, loss = 0.58734923\n","Iteration 78, loss = 0.58637181\n","Iteration 79, loss = 0.58625272\n","Iteration 80, loss = 0.58597125\n","Iteration 81, loss = 0.58597974\n","Iteration 82, loss = 0.58505754\n","Iteration 83, loss = 0.58528499\n","Iteration 84, loss = 0.58438733\n","Iteration 85, loss = 0.58472852\n","Iteration 86, loss = 0.58795707\n","Iteration 87, loss = 0.58516282\n","Iteration 88, loss = 0.58542473\n","Iteration 89, loss = 0.58426095\n","Iteration 90, loss = 0.58435416\n","Iteration 91, loss = 0.58553913\n","Iteration 92, loss = 0.58347404\n","Iteration 93, loss = 0.58625393\n","Iteration 94, loss = 0.58511678\n","Iteration 95, loss = 0.58516121\n","Iteration 96, loss = 0.58504571\n","Iteration 97, loss = 0.58535032\n","Iteration 98, loss = 0.58490001\n","Iteration 99, loss = 0.58647889\n","Iteration 100, loss = 0.58486616\n","Iteration 101, loss = 0.58449435\n","Iteration 102, loss = 0.58388849\n","Iteration 103, loss = 0.58493051\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.67865575\n","Iteration 2, loss = 0.64774119\n","Iteration 3, loss = 0.63313525\n","Iteration 4, loss = 0.62470710\n","Iteration 5, loss = 0.61878972\n","Iteration 6, loss = 0.61289744\n","Iteration 7, loss = 0.60924477\n","Iteration 8, loss = 0.60510117\n","Iteration 9, loss = 0.60274526\n","Iteration 10, loss = 0.60070370\n","Iteration 11, loss = 0.59936548\n","Iteration 12, loss = 0.59758706\n","Iteration 13, loss = 0.59659206\n","Iteration 14, loss = 0.59642776\n","Iteration 15, loss = 0.59517249\n","Iteration 16, loss = 0.59512970\n","Iteration 17, loss = 0.59459273\n","Iteration 18, loss = 0.59382326\n","Iteration 19, loss = 0.59331057\n","Iteration 20, loss = 0.59317074\n","Iteration 21, loss = 0.59329373\n","Iteration 22, loss = 0.59272448\n","Iteration 23, loss = 0.59320192\n","Iteration 24, loss = 0.59179519\n","Iteration 25, loss = 0.59408549\n","Iteration 26, loss = 0.59214892\n","Iteration 27, loss = 0.59232366\n","Iteration 28, loss = 0.59133781\n","Iteration 29, loss = 0.59127028\n","Iteration 30, loss = 0.59077022\n","Iteration 31, loss = 0.59139616\n","Iteration 32, loss = 0.59147845\n","Iteration 33, loss = 0.59185397\n","Iteration 34, loss = 0.59160208\n","Iteration 35, loss = 0.59075287\n","Iteration 36, loss = 0.59112958\n","Iteration 37, loss = 0.59084646\n","Iteration 38, loss = 0.59066463\n","Iteration 39, loss = 0.59206657\n","Iteration 40, loss = 0.59246722\n","Iteration 41, loss = 0.59144910\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.65936505\n","Iteration 2, loss = 0.63367679\n","Iteration 3, loss = 0.62310696\n","Iteration 4, loss = 0.61495359\n","Iteration 5, loss = 0.60998307\n","Iteration 6, loss = 0.60523459\n","Iteration 7, loss = 0.60221110\n","Iteration 8, loss = 0.60027808\n","Iteration 9, loss = 0.59781916\n","Iteration 10, loss = 0.59595409\n","Iteration 11, loss = 0.59544428\n","Iteration 12, loss = 0.59382876\n","Iteration 13, loss = 0.59442070\n","Iteration 14, loss = 0.59259499\n","Iteration 15, loss = 0.59203848\n","Iteration 16, loss = 0.59180882\n","Iteration 17, loss = 0.59122413\n","Iteration 18, loss = 0.59098202\n","Iteration 19, loss = 0.59102719\n","Iteration 20, loss = 0.58966913\n","Iteration 21, loss = 0.59031568\n","Iteration 22, loss = 0.59072921\n","Iteration 23, loss = 0.59038943\n","Iteration 24, loss = 0.58933411\n","Iteration 25, loss = 0.59008932\n","Iteration 26, loss = 0.59091974\n","Iteration 27, loss = 0.58875517\n","Iteration 28, loss = 0.59038717\n","Iteration 29, loss = 0.58835467\n","Iteration 30, loss = 0.58917541\n","Iteration 31, loss = 0.58981243\n","Iteration 32, loss = 0.58869690\n","Iteration 33, loss = 0.58872094\n","Iteration 34, loss = 0.58889479\n","Iteration 35, loss = 0.58979322\n","Iteration 36, loss = 0.58839617\n","Iteration 37, loss = 0.58876924\n","Iteration 38, loss = 0.58875745\n","Iteration 39, loss = 0.58888337\n","Iteration 40, loss = 0.58940580\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.66958444\n","Iteration 2, loss = 0.64236292\n","Iteration 3, loss = 0.63073369\n","Iteration 4, loss = 0.62377931\n","Iteration 5, loss = 0.61748646\n","Iteration 6, loss = 0.61238750\n","Iteration 7, loss = 0.60878884\n","Iteration 8, loss = 0.60593771\n","Iteration 9, loss = 0.60426707\n","Iteration 10, loss = 0.60084353\n","Iteration 11, loss = 0.60060575\n","Iteration 12, loss = 0.59869755\n","Iteration 13, loss = 0.59792594\n","Iteration 14, loss = 0.59839883\n","Iteration 15, loss = 0.59640754\n","Iteration 16, loss = 0.59503268\n","Iteration 17, loss = 0.59500948\n","Iteration 18, loss = 0.59566483\n","Iteration 19, loss = 0.59478874\n","Iteration 20, loss = 0.59381028\n","Iteration 21, loss = 0.59375024\n","Iteration 22, loss = 0.59398986\n","Iteration 23, loss = 0.59574650\n","Iteration 24, loss = 0.59341199\n","Iteration 25, loss = 0.59304588\n","Iteration 26, loss = 0.59284246\n","Iteration 27, loss = 0.59291080\n","Iteration 28, loss = 0.59208213\n","Iteration 29, loss = 0.59210833\n","Iteration 30, loss = 0.59226776\n","Iteration 31, loss = 0.59327617\n","Iteration 32, loss = 0.59249076\n","Iteration 33, loss = 0.59172102\n","Iteration 34, loss = 0.59249202\n","Iteration 35, loss = 0.59188074\n","Iteration 36, loss = 0.59156949\n","Iteration 37, loss = 0.59220422\n","Iteration 38, loss = 0.59105121\n","Iteration 39, loss = 0.59118004\n","Iteration 40, loss = 0.59164023\n","Iteration 41, loss = 0.58983944\n","Iteration 42, loss = 0.59271396\n","Iteration 43, loss = 0.59006343\n","Iteration 44, loss = 0.59033866\n","Iteration 45, loss = 0.59050013\n","Iteration 46, loss = 0.58989453\n","Iteration 47, loss = 0.59089548\n","Iteration 48, loss = 0.59269230\n","Iteration 49, loss = 0.59081068\n","Iteration 50, loss = 0.58959351\n","Iteration 51, loss = 0.59055897\n","Iteration 52, loss = 0.58965001\n","Iteration 53, loss = 0.58937190\n","Iteration 54, loss = 0.58990667\n","Iteration 55, loss = 0.59038381\n","Iteration 56, loss = 0.59024331\n","Iteration 57, loss = 0.58820729\n","Iteration 58, loss = 0.58928867\n","Iteration 59, loss = 0.58875351\n","Iteration 60, loss = 0.58994214\n","Iteration 61, loss = 0.58930538\n","Iteration 62, loss = 0.58877679\n","Iteration 63, loss = 0.58978893\n","Iteration 64, loss = 0.58889400\n","Iteration 65, loss = 0.58952173\n","Iteration 66, loss = 0.58898156\n","Iteration 67, loss = 0.58843781\n","Iteration 68, loss = 0.58793689\n","Iteration 69, loss = 0.58837003\n","Iteration 70, loss = 0.58833625\n","Iteration 71, loss = 0.58791982\n","Iteration 72, loss = 0.58851309\n","Iteration 73, loss = 0.58959302\n","Iteration 74, loss = 0.58898633\n","Iteration 75, loss = 0.58838091\n","Iteration 76, loss = 0.58884339\n","Iteration 77, loss = 0.58818533\n","Iteration 78, loss = 0.58780743\n","Iteration 79, loss = 0.58791599\n","Iteration 80, loss = 0.58807530\n","Iteration 81, loss = 0.58819808\n","Iteration 82, loss = 0.58827405\n","Iteration 83, loss = 0.58864812\n","Iteration 84, loss = 0.58841512\n","Iteration 85, loss = 0.58820260\n","Iteration 86, loss = 0.58839748\n","Iteration 87, loss = 0.58895155\n","Iteration 88, loss = 0.58749719\n","Iteration 89, loss = 0.58808350\n","Iteration 90, loss = 0.58749409\n","Iteration 91, loss = 0.58795993\n","Iteration 92, loss = 0.58817639\n","Iteration 93, loss = 0.58748225\n","Iteration 94, loss = 0.58825444\n","Iteration 95, loss = 0.58712021\n","Iteration 96, loss = 0.58755031\n","Iteration 97, loss = 0.58686288\n","Iteration 98, loss = 0.58740852\n","Iteration 99, loss = 0.58766033\n","Iteration 100, loss = 0.58668622\n","Iteration 101, loss = 0.58778960\n","Iteration 102, loss = 0.58677591\n","Iteration 103, loss = 0.58702671\n","Iteration 104, loss = 0.58686589\n","Iteration 105, loss = 0.58630155\n","Iteration 106, loss = 0.58652253\n","Iteration 107, loss = 0.58728127\n","Iteration 108, loss = 0.58669486\n","Iteration 109, loss = 0.58540361\n","Iteration 110, loss = 0.58861436\n","Iteration 111, loss = 0.58786431\n","Iteration 112, loss = 0.58813640\n","Iteration 113, loss = 0.58716309\n","Iteration 114, loss = 0.58807352\n","Iteration 115, loss = 0.58732145\n","Iteration 116, loss = 0.58719562\n","Iteration 117, loss = 0.58700297\n","Iteration 118, loss = 0.58680974\n","Iteration 119, loss = 0.58619360\n","Iteration 120, loss = 0.58702879\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.67267008\n","Iteration 2, loss = 0.63976990\n","Iteration 3, loss = 0.62621806\n","Iteration 4, loss = 0.61800208\n","Iteration 5, loss = 0.61206163\n","Iteration 6, loss = 0.60694961\n","Iteration 7, loss = 0.60328900\n","Iteration 8, loss = 0.60056617\n","Iteration 9, loss = 0.59786826\n","Iteration 10, loss = 0.59692864\n","Iteration 11, loss = 0.59511570\n","Iteration 12, loss = 0.59512169\n","Iteration 13, loss = 0.59336512\n","Iteration 14, loss = 0.59194597\n","Iteration 15, loss = 0.59159403\n","Iteration 16, loss = 0.59140686\n","Iteration 17, loss = 0.59104083\n","Iteration 18, loss = 0.59068225\n","Iteration 19, loss = 0.59016035\n","Iteration 20, loss = 0.59398324\n","Iteration 21, loss = 0.58966925\n","Iteration 22, loss = 0.59004549\n","Iteration 23, loss = 0.58928690\n","Iteration 24, loss = 0.59016007\n","Iteration 25, loss = 0.58872361\n","Iteration 26, loss = 0.59044331\n","Iteration 27, loss = 0.58874098\n","Iteration 28, loss = 0.58880171\n","Iteration 29, loss = 0.58837124\n","Iteration 30, loss = 0.58859594\n","Iteration 31, loss = 0.58775677\n","Iteration 32, loss = 0.58833470\n","Iteration 33, loss = 0.58689885\n","Iteration 34, loss = 0.58771955\n","Iteration 35, loss = 0.58768960\n","Iteration 36, loss = 0.58659192\n","Iteration 37, loss = 0.58691500\n","Iteration 38, loss = 0.58881661\n","Iteration 39, loss = 0.58749017\n","Iteration 40, loss = 0.58795680\n","Iteration 41, loss = 0.58684092\n","Iteration 42, loss = 0.58680903\n","Iteration 43, loss = 0.58747967\n","Iteration 44, loss = 0.58627388\n","Iteration 45, loss = 0.58684614\n","Iteration 46, loss = 0.58675234\n","Iteration 47, loss = 0.58682642\n","Iteration 48, loss = 0.58774232\n","Iteration 49, loss = 0.58622747\n","Iteration 50, loss = 0.58564162\n","Iteration 51, loss = 0.58675935\n","Iteration 52, loss = 0.58628707\n","Iteration 53, loss = 0.58685096\n","Iteration 54, loss = 0.58530714\n","Iteration 55, loss = 0.58505364\n","Iteration 56, loss = 0.58676441\n","Iteration 57, loss = 0.58599649\n","Iteration 58, loss = 0.58578674\n","Iteration 59, loss = 0.58511459\n","Iteration 60, loss = 0.58504376\n","Iteration 61, loss = 0.58563138\n","Iteration 62, loss = 0.58557722\n","Iteration 63, loss = 0.58566550\n","Iteration 64, loss = 0.58566366\n","Iteration 65, loss = 0.58540769\n","Iteration 66, loss = 0.58540316\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.68769913\n","Iteration 2, loss = 0.65191600\n","Iteration 3, loss = 0.63591980\n","Iteration 4, loss = 0.62454731\n","Iteration 5, loss = 0.61695098\n","Iteration 6, loss = 0.61006657\n","Iteration 7, loss = 0.60634908\n","Iteration 8, loss = 0.60046352\n","Iteration 9, loss = 0.59727782\n","Iteration 10, loss = 0.59489130\n","Iteration 11, loss = 0.59258483\n","Iteration 12, loss = 0.59079165\n","Iteration 13, loss = 0.58968705\n","Iteration 14, loss = 0.58932797\n","Iteration 15, loss = 0.58748728\n","Iteration 16, loss = 0.58776266\n","Iteration 17, loss = 0.58668243\n","Iteration 18, loss = 0.58700520\n","Iteration 19, loss = 0.58577418\n","Iteration 20, loss = 0.58618444\n","Iteration 21, loss = 0.58521339\n","Iteration 22, loss = 0.58529548\n","Iteration 23, loss = 0.58651090\n","Iteration 24, loss = 0.58478164\n","Iteration 25, loss = 0.58472748\n","Iteration 26, loss = 0.58612596\n","Iteration 27, loss = 0.58468622\n","Iteration 28, loss = 0.58547449\n","Iteration 29, loss = 0.58413334\n","Iteration 30, loss = 0.58455336\n","Iteration 31, loss = 0.58547448\n","Iteration 32, loss = 0.58333740\n","Iteration 33, loss = 0.58344471\n","Iteration 34, loss = 0.58397284\n","Iteration 35, loss = 0.58406200\n","Iteration 36, loss = 0.58351695\n","Iteration 37, loss = 0.58342089\n","Iteration 38, loss = 0.58283143\n","Iteration 39, loss = 0.58470665\n","Iteration 40, loss = 0.58322356\n","Iteration 41, loss = 0.58287877\n","Iteration 42, loss = 0.58268394\n","Iteration 43, loss = 0.58299699\n","Iteration 44, loss = 0.58280027\n","Iteration 45, loss = 0.58303513\n","Iteration 46, loss = 0.58298877\n","Iteration 47, loss = 0.58257273\n","Iteration 48, loss = 0.58264296\n","Iteration 49, loss = 0.58250065\n","Iteration 50, loss = 0.58327812\n","Iteration 51, loss = 0.58245062\n","Iteration 52, loss = 0.58313875\n","Iteration 53, loss = 0.58216555\n","Iteration 54, loss = 0.58249737\n","Iteration 55, loss = 0.58280784\n","Iteration 56, loss = 0.58265446\n","Iteration 57, loss = 0.58205946\n","Iteration 58, loss = 0.58292205\n","Iteration 59, loss = 0.58220752\n","Iteration 60, loss = 0.58158493\n","Iteration 61, loss = 0.58186863\n","Iteration 62, loss = 0.58091334\n","Iteration 63, loss = 0.58299588\n","Iteration 64, loss = 0.58145812\n","Iteration 65, loss = 0.58369717\n","Iteration 66, loss = 0.58142675\n","Iteration 67, loss = 0.58138016\n","Iteration 68, loss = 0.58263420\n","Iteration 69, loss = 0.58070536\n","Iteration 70, loss = 0.58128187\n","Iteration 71, loss = 0.58038545\n","Iteration 72, loss = 0.58171752\n","Iteration 73, loss = 0.58127304\n","Iteration 74, loss = 0.58127438\n","Iteration 75, loss = 0.58053819\n","Iteration 76, loss = 0.58120596\n","Iteration 77, loss = 0.58048357\n","Iteration 78, loss = 0.58191926\n","Iteration 79, loss = 0.58000135\n","Iteration 80, loss = 0.58046013\n","Iteration 81, loss = 0.58146958\n","Iteration 82, loss = 0.58008350\n","Iteration 83, loss = 0.58050723\n","Iteration 84, loss = 0.58054439\n","Iteration 85, loss = 0.58036367\n","Iteration 86, loss = 0.58084320\n","Iteration 87, loss = 0.57930342\n","Iteration 88, loss = 0.58102328\n","Iteration 89, loss = 0.58078427\n","Iteration 90, loss = 0.57890373\n","Iteration 91, loss = 0.58048489\n","Iteration 92, loss = 0.58137071\n","Iteration 93, loss = 0.58083584\n","Iteration 94, loss = 0.57983524\n","Iteration 95, loss = 0.57991696\n","Iteration 96, loss = 0.58045254\n","Iteration 97, loss = 0.57982969\n","Iteration 98, loss = 0.57962367\n","Iteration 99, loss = 0.57964208\n","Iteration 100, loss = 0.57903164\n","Iteration 101, loss = 0.58013523\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.67118147\n","Iteration 2, loss = 0.64079744\n","Iteration 3, loss = 0.62854317\n","Iteration 4, loss = 0.62100318\n","Iteration 5, loss = 0.61462060\n","Iteration 6, loss = 0.60974211\n","Iteration 7, loss = 0.60633544\n","Iteration 8, loss = 0.60352583\n","Iteration 9, loss = 0.60097479\n","Iteration 10, loss = 0.59981766\n","Iteration 11, loss = 0.59790218\n","Iteration 12, loss = 0.59765860\n","Iteration 13, loss = 0.59478376\n","Iteration 14, loss = 0.59471895\n","Iteration 15, loss = 0.59344919\n","Iteration 16, loss = 0.59231873\n","Iteration 17, loss = 0.59284989\n","Iteration 18, loss = 0.59222805\n","Iteration 19, loss = 0.59142931\n","Iteration 20, loss = 0.59127973\n","Iteration 21, loss = 0.59083501\n","Iteration 22, loss = 0.59099562\n","Iteration 23, loss = 0.59066769\n","Iteration 24, loss = 0.59091774\n","Iteration 25, loss = 0.59089816\n","Iteration 26, loss = 0.59101857\n","Iteration 27, loss = 0.59024114\n","Iteration 28, loss = 0.59105163\n","Iteration 29, loss = 0.59020884\n","Iteration 30, loss = 0.59016918\n","Iteration 31, loss = 0.59041479\n","Iteration 32, loss = 0.59017942\n","Iteration 33, loss = 0.59064235\n","Iteration 34, loss = 0.58921450\n","Iteration 35, loss = 0.58931472\n","Iteration 36, loss = 0.58968448\n","Iteration 37, loss = 0.58898230\n","Iteration 38, loss = 0.58916515\n","Iteration 39, loss = 0.58884735\n","Iteration 40, loss = 0.58897083\n","Iteration 41, loss = 0.58848019\n","Iteration 42, loss = 0.58903136\n","Iteration 43, loss = 0.58893958\n","Iteration 44, loss = 0.58860793\n","Iteration 45, loss = 0.58812767\n","Iteration 46, loss = 0.58789975\n","Iteration 47, loss = 0.58801028\n","Iteration 48, loss = 0.58765875\n","Iteration 49, loss = 0.58867980\n","Iteration 50, loss = 0.58841358\n","Iteration 51, loss = 0.58743980\n","Iteration 52, loss = 0.58797259\n","Iteration 53, loss = 0.58776849\n","Iteration 54, loss = 0.58839935\n","Iteration 55, loss = 0.58935697\n","Iteration 56, loss = 0.58817958\n","Iteration 57, loss = 0.58803812\n","Iteration 58, loss = 0.58748946\n","Iteration 59, loss = 0.58749541\n","Iteration 60, loss = 0.58787441\n","Iteration 61, loss = 0.58759820\n","Iteration 62, loss = 0.58774839\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","평균정확률: 68.83740074306112 %\n","Iteration 1, loss = 0.66344596\n","Iteration 2, loss = 0.63621106\n","Iteration 3, loss = 0.62478471\n","Iteration 4, loss = 0.61656095\n","Iteration 5, loss = 0.61100200\n","Iteration 6, loss = 0.60548049\n","Iteration 7, loss = 0.60099155\n","Iteration 8, loss = 0.59727501\n","Iteration 9, loss = 0.59547643\n","Iteration 10, loss = 0.59451243\n","Iteration 11, loss = 0.59365204\n","Iteration 12, loss = 0.59291117\n","Iteration 13, loss = 0.59278289\n","Iteration 14, loss = 0.59161755\n","Iteration 15, loss = 0.59139892\n","Iteration 16, loss = 0.59129840\n","Iteration 17, loss = 0.59065656\n","Iteration 18, loss = 0.59027606\n","Iteration 19, loss = 0.59127207\n","Iteration 20, loss = 0.59073959\n","Iteration 21, loss = 0.59126007\n","Iteration 22, loss = 0.58984326\n","Iteration 23, loss = 0.58941527\n","Iteration 24, loss = 0.58955026\n","Iteration 25, loss = 0.58788704\n","Iteration 26, loss = 0.58956288\n","Iteration 27, loss = 0.58948345\n","Iteration 28, loss = 0.58883380\n","Iteration 29, loss = 0.58851198\n","Iteration 30, loss = 0.58879209\n","Iteration 31, loss = 0.58889540\n","Iteration 32, loss = 0.58850703\n","Iteration 33, loss = 0.58839607\n","Iteration 34, loss = 0.58905820\n","Iteration 35, loss = 0.58867459\n","Iteration 36, loss = 0.58747641\n","Iteration 37, loss = 0.58844114\n","Iteration 38, loss = 0.58755669\n","Iteration 39, loss = 0.58743832\n","Iteration 40, loss = 0.58683531\n","Iteration 41, loss = 0.58895393\n","Iteration 42, loss = 0.58887094\n","Iteration 43, loss = 0.58617384\n","Iteration 44, loss = 0.58857177\n","Iteration 45, loss = 0.58698472\n","Iteration 46, loss = 0.58786369\n","Iteration 47, loss = 0.58657186\n","Iteration 48, loss = 0.58663311\n","Iteration 49, loss = 0.58691316\n","Iteration 50, loss = 0.58745019\n","Iteration 51, loss = 0.58724241\n","Iteration 52, loss = 0.58660612\n","Iteration 53, loss = 0.58791391\n","Iteration 54, loss = 0.58710468\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.68650491\n","Iteration 2, loss = 0.65475653\n","Iteration 3, loss = 0.64120020\n","Iteration 4, loss = 0.63283843\n","Iteration 5, loss = 0.62643509\n","Iteration 6, loss = 0.62002218\n","Iteration 7, loss = 0.61472194\n","Iteration 8, loss = 0.60958570\n","Iteration 9, loss = 0.60540295\n","Iteration 10, loss = 0.60148293\n","Iteration 11, loss = 0.59964713\n","Iteration 12, loss = 0.59582117\n","Iteration 13, loss = 0.59461560\n","Iteration 14, loss = 0.59223384\n","Iteration 15, loss = 0.59302065\n","Iteration 16, loss = 0.59063615\n","Iteration 17, loss = 0.59005999\n","Iteration 18, loss = 0.58866514\n","Iteration 19, loss = 0.59080306\n","Iteration 20, loss = 0.58931057\n","Iteration 21, loss = 0.58801319\n","Iteration 22, loss = 0.58742490\n","Iteration 23, loss = 0.58780666\n","Iteration 24, loss = 0.58684743\n","Iteration 25, loss = 0.58754654\n","Iteration 26, loss = 0.58654142\n","Iteration 27, loss = 0.58586382\n","Iteration 28, loss = 0.58658374\n","Iteration 29, loss = 0.58806831\n","Iteration 30, loss = 0.58636631\n","Iteration 31, loss = 0.58661412\n","Iteration 32, loss = 0.58675397\n","Iteration 33, loss = 0.58652794\n","Iteration 34, loss = 0.58507491\n","Iteration 35, loss = 0.58491939\n","Iteration 36, loss = 0.58479436\n","Iteration 37, loss = 0.58465971\n","Iteration 38, loss = 0.58567650\n","Iteration 39, loss = 0.58478976\n","Iteration 40, loss = 0.58472307\n","Iteration 41, loss = 0.58498316\n","Iteration 42, loss = 0.58506410\n","Iteration 43, loss = 0.58472161\n","Iteration 44, loss = 0.58443334\n","Iteration 45, loss = 0.58491265\n","Iteration 46, loss = 0.58385301\n","Iteration 47, loss = 0.58462270\n","Iteration 48, loss = 0.58341256\n","Iteration 49, loss = 0.58479487\n","Iteration 50, loss = 0.58336380\n","Iteration 51, loss = 0.58363246\n","Iteration 52, loss = 0.58366203\n","Iteration 53, loss = 0.58497545\n","Iteration 54, loss = 0.58348977\n","Iteration 55, loss = 0.58495649\n","Iteration 56, loss = 0.58336974\n","Iteration 57, loss = 0.58357545\n","Iteration 58, loss = 0.58238220\n","Iteration 59, loss = 0.58270593\n","Iteration 60, loss = 0.58258280\n","Iteration 61, loss = 0.58304942\n","Iteration 62, loss = 0.58233852\n","Iteration 63, loss = 0.58246860\n","Iteration 64, loss = 0.58295210\n","Iteration 65, loss = 0.58234134\n","Iteration 66, loss = 0.58252353\n","Iteration 67, loss = 0.58175604\n","Iteration 68, loss = 0.58207524\n","Iteration 69, loss = 0.58189461\n","Iteration 70, loss = 0.58236107\n","Iteration 71, loss = 0.58237745\n","Iteration 72, loss = 0.58260018\n","Iteration 73, loss = 0.58234871\n","Iteration 74, loss = 0.58221622\n","Iteration 75, loss = 0.58089158\n","Iteration 76, loss = 0.58173755\n","Iteration 77, loss = 0.58022820\n","Iteration 78, loss = 0.58222663\n","Iteration 79, loss = 0.58074755\n","Iteration 80, loss = 0.58125508\n","Iteration 81, loss = 0.58149574\n","Iteration 82, loss = 0.58212020\n","Iteration 83, loss = 0.58146349\n","Iteration 84, loss = 0.58159653\n","Iteration 85, loss = 0.58105979\n","Iteration 86, loss = 0.57982660\n","Iteration 87, loss = 0.58123395\n","Iteration 88, loss = 0.58103297\n","Iteration 89, loss = 0.58109030\n","Iteration 90, loss = 0.58080208\n","Iteration 91, loss = 0.57994574\n","Iteration 92, loss = 0.58063957\n","Iteration 93, loss = 0.58026770\n","Iteration 94, loss = 0.57939420\n","Iteration 95, loss = 0.58025779\n","Iteration 96, loss = 0.57901593\n","Iteration 97, loss = 0.58009221\n","Iteration 98, loss = 0.57942194\n","Iteration 99, loss = 0.58032453\n","Iteration 100, loss = 0.57927408\n","Iteration 101, loss = 0.58044479\n","Iteration 102, loss = 0.57974401\n","Iteration 103, loss = 0.57981213\n","Iteration 104, loss = 0.57911311\n","Iteration 105, loss = 0.58052521\n","Iteration 106, loss = 0.57918663\n","Iteration 107, loss = 0.57852126\n","Iteration 108, loss = 0.57806808\n","Iteration 109, loss = 0.58033650\n","Iteration 110, loss = 0.57912894\n","Iteration 111, loss = 0.58041636\n","Iteration 112, loss = 0.57971203\n","Iteration 113, loss = 0.57857803\n","Iteration 114, loss = 0.57812445\n","Iteration 115, loss = 0.58025134\n","Iteration 116, loss = 0.57942544\n","Iteration 117, loss = 0.57967491\n","Iteration 118, loss = 0.57895852\n","Iteration 119, loss = 0.57819554\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.66002002\n","Iteration 2, loss = 0.63471819\n","Iteration 3, loss = 0.62460564\n","Iteration 4, loss = 0.61716875\n","Iteration 5, loss = 0.61110813\n","Iteration 6, loss = 0.60620559\n","Iteration 7, loss = 0.60134619\n","Iteration 8, loss = 0.59825892\n","Iteration 9, loss = 0.59701404\n","Iteration 10, loss = 0.59496686\n","Iteration 11, loss = 0.59186327\n","Iteration 12, loss = 0.59118946\n","Iteration 13, loss = 0.58990575\n","Iteration 14, loss = 0.58924270\n","Iteration 15, loss = 0.58933949\n","Iteration 16, loss = 0.58775312\n","Iteration 17, loss = 0.58838643\n","Iteration 18, loss = 0.58817809\n","Iteration 19, loss = 0.58755363\n","Iteration 20, loss = 0.58663444\n","Iteration 21, loss = 0.58690153\n","Iteration 22, loss = 0.58698365\n","Iteration 23, loss = 0.58698629\n","Iteration 24, loss = 0.58632283\n","Iteration 25, loss = 0.58608815\n","Iteration 26, loss = 0.58571817\n","Iteration 27, loss = 0.58939952\n","Iteration 28, loss = 0.58540657\n","Iteration 29, loss = 0.58508337\n","Iteration 30, loss = 0.58561196\n","Iteration 31, loss = 0.58597536\n","Iteration 32, loss = 0.58570361\n","Iteration 33, loss = 0.58607441\n","Iteration 34, loss = 0.58584204\n","Iteration 35, loss = 0.58465728\n","Iteration 36, loss = 0.58519629\n","Iteration 37, loss = 0.58605347\n","Iteration 38, loss = 0.58352927\n","Iteration 39, loss = 0.58553301\n","Iteration 40, loss = 0.58366948\n","Iteration 41, loss = 0.58442005\n","Iteration 42, loss = 0.58576017\n","Iteration 43, loss = 0.58428362\n","Iteration 44, loss = 0.58402013\n","Iteration 45, loss = 0.58541685\n","Iteration 46, loss = 0.58321259\n","Iteration 47, loss = 0.58369567\n","Iteration 48, loss = 0.58403923\n","Iteration 49, loss = 0.58392288\n","Iteration 50, loss = 0.58378468\n","Iteration 51, loss = 0.58367221\n","Iteration 52, loss = 0.58277453\n","Iteration 53, loss = 0.58445318\n","Iteration 54, loss = 0.58235713\n","Iteration 55, loss = 0.58281655\n","Iteration 56, loss = 0.58315530\n","Iteration 57, loss = 0.58288973\n","Iteration 58, loss = 0.58324290\n","Iteration 59, loss = 0.58331109\n","Iteration 60, loss = 0.58377478\n","Iteration 61, loss = 0.58238908\n","Iteration 62, loss = 0.58196416\n","Iteration 63, loss = 0.58116515\n","Iteration 64, loss = 0.58203402\n","Iteration 65, loss = 0.58273642\n","Iteration 66, loss = 0.58181946\n","Iteration 67, loss = 0.58003615\n","Iteration 68, loss = 0.58283128\n","Iteration 69, loss = 0.58242706\n","Iteration 70, loss = 0.58209091\n","Iteration 71, loss = 0.58143777\n","Iteration 72, loss = 0.58173081\n","Iteration 73, loss = 0.58203309\n","Iteration 74, loss = 0.58147417\n","Iteration 75, loss = 0.58054532\n","Iteration 76, loss = 0.58238910\n","Iteration 77, loss = 0.58067341\n","Iteration 78, loss = 0.58100682\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.65686908\n","Iteration 2, loss = 0.63593295\n","Iteration 3, loss = 0.62724176\n","Iteration 4, loss = 0.62072508\n","Iteration 5, loss = 0.61501144\n","Iteration 6, loss = 0.61008603\n","Iteration 7, loss = 0.60627044\n","Iteration 8, loss = 0.60374761\n","Iteration 9, loss = 0.60012762\n","Iteration 10, loss = 0.59775183\n","Iteration 11, loss = 0.59608694\n","Iteration 12, loss = 0.59554856\n","Iteration 13, loss = 0.59331338\n","Iteration 14, loss = 0.59370107\n","Iteration 15, loss = 0.59128178\n","Iteration 16, loss = 0.58996355\n","Iteration 17, loss = 0.59003839\n","Iteration 18, loss = 0.58923085\n","Iteration 19, loss = 0.58891013\n","Iteration 20, loss = 0.58821136\n","Iteration 21, loss = 0.58862214\n","Iteration 22, loss = 0.58835595\n","Iteration 23, loss = 0.58972011\n","Iteration 24, loss = 0.58782312\n","Iteration 25, loss = 0.58698272\n","Iteration 26, loss = 0.58751217\n","Iteration 27, loss = 0.58776879\n","Iteration 28, loss = 0.58805048\n","Iteration 29, loss = 0.58734226\n","Iteration 30, loss = 0.58614580\n","Iteration 31, loss = 0.58561983\n","Iteration 32, loss = 0.58602097\n","Iteration 33, loss = 0.58668920\n","Iteration 34, loss = 0.58704061\n","Iteration 35, loss = 0.58652694\n","Iteration 36, loss = 0.58545918\n","Iteration 37, loss = 0.58619629\n","Iteration 38, loss = 0.58637900\n","Iteration 39, loss = 0.58593513\n","Iteration 40, loss = 0.58530187\n","Iteration 41, loss = 0.58641319\n","Iteration 42, loss = 0.58484723\n","Iteration 43, loss = 0.58502838\n","Iteration 44, loss = 0.58557217\n","Iteration 45, loss = 0.58581710\n","Iteration 46, loss = 0.58573629\n","Iteration 47, loss = 0.58649813\n","Iteration 48, loss = 0.58403662\n","Iteration 49, loss = 0.58519739\n","Iteration 50, loss = 0.58413905\n","Iteration 51, loss = 0.58402537\n","Iteration 52, loss = 0.58356855\n","Iteration 53, loss = 0.58400976\n","Iteration 54, loss = 0.58454155\n","Iteration 55, loss = 0.58456637\n","Iteration 56, loss = 0.58580301\n","Iteration 57, loss = 0.58580430\n","Iteration 58, loss = 0.58516733\n","Iteration 59, loss = 0.58454142\n","Iteration 60, loss = 0.58376586\n","Iteration 61, loss = 0.58494820\n","Iteration 62, loss = 0.58528483\n","Iteration 63, loss = 0.58374561\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.65802805\n","Iteration 2, loss = 0.63246678\n","Iteration 3, loss = 0.62304148\n","Iteration 4, loss = 0.61796666\n","Iteration 5, loss = 0.61240211\n","Iteration 6, loss = 0.60750340\n","Iteration 7, loss = 0.60408898\n","Iteration 8, loss = 0.60060756\n","Iteration 9, loss = 0.59801204\n","Iteration 10, loss = 0.59547735\n","Iteration 11, loss = 0.59393983\n","Iteration 12, loss = 0.59383847\n","Iteration 13, loss = 0.59317057\n","Iteration 14, loss = 0.59296546\n","Iteration 15, loss = 0.59244412\n","Iteration 16, loss = 0.59140846\n","Iteration 17, loss = 0.59306484\n","Iteration 18, loss = 0.59037688\n","Iteration 19, loss = 0.59077397\n","Iteration 20, loss = 0.59099534\n","Iteration 21, loss = 0.59100989\n","Iteration 22, loss = 0.59053392\n","Iteration 23, loss = 0.59124678\n","Iteration 24, loss = 0.59066833\n","Iteration 25, loss = 0.59212013\n","Iteration 26, loss = 0.59060593\n","Iteration 27, loss = 0.58954519\n","Iteration 28, loss = 0.59048024\n","Iteration 29, loss = 0.58884924\n","Iteration 30, loss = 0.58946031\n","Iteration 31, loss = 0.58872804\n","Iteration 32, loss = 0.58958160\n","Iteration 33, loss = 0.58878154\n","Iteration 34, loss = 0.58812608\n","Iteration 35, loss = 0.59014232\n","Iteration 36, loss = 0.58808507\n","Iteration 37, loss = 0.58821359\n","Iteration 38, loss = 0.58761431\n","Iteration 39, loss = 0.58856423\n","Iteration 40, loss = 0.58743727\n","Iteration 41, loss = 0.58793839\n","Iteration 42, loss = 0.58737829\n","Iteration 43, loss = 0.58772502\n","Iteration 44, loss = 0.58768343\n","Iteration 45, loss = 0.58717444\n","Iteration 46, loss = 0.58636823\n","Iteration 47, loss = 0.58776701\n","Iteration 48, loss = 0.58548872\n","Iteration 49, loss = 0.58734535\n","Iteration 50, loss = 0.58616061\n","Iteration 51, loss = 0.58686178\n","Iteration 52, loss = 0.58632315\n","Iteration 53, loss = 0.58644147\n","Iteration 54, loss = 0.58558589\n","Iteration 55, loss = 0.58665635\n","Iteration 56, loss = 0.58635083\n","Iteration 57, loss = 0.58572810\n","Iteration 58, loss = 0.58671426\n","Iteration 59, loss = 0.58623046\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.67052784\n","Iteration 2, loss = 0.63729169\n","Iteration 3, loss = 0.62519320\n","Iteration 4, loss = 0.61766987\n","Iteration 5, loss = 0.61045512\n","Iteration 6, loss = 0.60535078\n","Iteration 7, loss = 0.60461889\n","Iteration 8, loss = 0.59975855\n","Iteration 9, loss = 0.59691720\n","Iteration 10, loss = 0.59695312\n","Iteration 11, loss = 0.59455331\n","Iteration 12, loss = 0.59499189\n","Iteration 13, loss = 0.59327319\n","Iteration 14, loss = 0.59702638\n","Iteration 15, loss = 0.59342139\n","Iteration 16, loss = 0.59242214\n","Iteration 17, loss = 0.59347238\n","Iteration 18, loss = 0.59237888\n","Iteration 19, loss = 0.59357628\n","Iteration 20, loss = 0.59110539\n","Iteration 21, loss = 0.59274197\n","Iteration 22, loss = 0.59129920\n","Iteration 23, loss = 0.59126363\n","Iteration 24, loss = 0.59139819\n","Iteration 25, loss = 0.59206899\n","Iteration 26, loss = 0.59221375\n","Iteration 27, loss = 0.59019005\n","Iteration 28, loss = 0.58986889\n","Iteration 29, loss = 0.59032233\n","Iteration 30, loss = 0.59275586\n","Iteration 31, loss = 0.58939259\n","Iteration 32, loss = 0.58994159\n","Iteration 33, loss = 0.58929852\n","Iteration 34, loss = 0.59078488\n","Iteration 35, loss = 0.59010045\n","Iteration 36, loss = 0.59200185\n","Iteration 37, loss = 0.58932368\n","Iteration 38, loss = 0.58911852\n","Iteration 39, loss = 0.59006906\n","Iteration 40, loss = 0.58954759\n","Iteration 41, loss = 0.59021519\n","Iteration 42, loss = 0.58837726\n","Iteration 43, loss = 0.59186374\n","Iteration 44, loss = 0.58829121\n","Iteration 45, loss = 0.59107168\n","Iteration 46, loss = 0.58883234\n","Iteration 47, loss = 0.58825616\n","Iteration 48, loss = 0.58947095\n","Iteration 49, loss = 0.58924545\n","Iteration 50, loss = 0.58800306\n","Iteration 51, loss = 0.58857846\n","Iteration 52, loss = 0.58800651\n","Iteration 53, loss = 0.58731140\n","Iteration 54, loss = 0.58796381\n","Iteration 55, loss = 0.58699185\n","Iteration 56, loss = 0.58964686\n","Iteration 57, loss = 0.58680593\n","Iteration 58, loss = 0.58792753\n","Iteration 59, loss = 0.58773154\n","Iteration 60, loss = 0.58689495\n","Iteration 61, loss = 0.58683143\n","Iteration 62, loss = 0.58780965\n","Iteration 63, loss = 0.58650264\n","Iteration 64, loss = 0.58693787\n","Iteration 65, loss = 0.58651688\n","Iteration 66, loss = 0.58669349\n","Iteration 67, loss = 0.58701004\n","Iteration 68, loss = 0.58679668\n","Iteration 69, loss = 0.58615498\n","Iteration 70, loss = 0.58901004\n","Iteration 71, loss = 0.58658337\n","Iteration 72, loss = 0.58660248\n","Iteration 73, loss = 0.58676712\n","Iteration 74, loss = 0.58671959\n","Iteration 75, loss = 0.58651138\n","Iteration 76, loss = 0.58590891\n","Iteration 77, loss = 0.58647825\n","Iteration 78, loss = 0.58530109\n","Iteration 79, loss = 0.58797291\n","Iteration 80, loss = 0.58650952\n","Iteration 81, loss = 0.58630469\n","Iteration 82, loss = 0.58655352\n","Iteration 83, loss = 0.58503855\n","Iteration 84, loss = 0.58552869\n","Iteration 85, loss = 0.58613717\n","Iteration 86, loss = 0.58574921\n","Iteration 87, loss = 0.58660824\n","Iteration 88, loss = 0.58639831\n","Iteration 89, loss = 0.58509507\n","Iteration 90, loss = 0.58587347\n","Iteration 91, loss = 0.58616707\n","Iteration 92, loss = 0.58544505\n","Iteration 93, loss = 0.58502329\n","Iteration 94, loss = 0.58486911\n","Iteration 95, loss = 0.58501780\n","Iteration 96, loss = 0.58367279\n","Iteration 97, loss = 0.58562267\n","Iteration 98, loss = 0.58576894\n","Iteration 99, loss = 0.58460666\n","Iteration 100, loss = 0.58517372\n","Iteration 101, loss = 0.58465681\n","Iteration 102, loss = 0.58436460\n","Iteration 103, loss = 0.58450948\n","Iteration 104, loss = 0.58417974\n","Iteration 105, loss = 0.58377559\n","Iteration 106, loss = 0.58387368\n","Iteration 107, loss = 0.58395044\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.67974269\n","Iteration 2, loss = 0.64841054\n","Iteration 3, loss = 0.63642867\n","Iteration 4, loss = 0.62853158\n","Iteration 5, loss = 0.62232705\n","Iteration 6, loss = 0.61753615\n","Iteration 7, loss = 0.61224140\n","Iteration 8, loss = 0.60710757\n","Iteration 9, loss = 0.60369721\n","Iteration 10, loss = 0.60176606\n","Iteration 11, loss = 0.59950610\n","Iteration 12, loss = 0.59662921\n","Iteration 13, loss = 0.59540569\n","Iteration 14, loss = 0.59466647\n","Iteration 15, loss = 0.59465243\n","Iteration 16, loss = 0.59321827\n","Iteration 17, loss = 0.59350964\n","Iteration 18, loss = 0.59348196\n","Iteration 19, loss = 0.59168270\n","Iteration 20, loss = 0.59143749\n","Iteration 21, loss = 0.59127668\n","Iteration 22, loss = 0.59009713\n","Iteration 23, loss = 0.59117715\n","Iteration 24, loss = 0.59120612\n","Iteration 25, loss = 0.58977818\n","Iteration 26, loss = 0.59008010\n","Iteration 27, loss = 0.59092882\n","Iteration 28, loss = 0.59034517\n","Iteration 29, loss = 0.59008867\n","Iteration 30, loss = 0.59004637\n","Iteration 31, loss = 0.58864534\n","Iteration 32, loss = 0.58961981\n","Iteration 33, loss = 0.59149078\n","Iteration 34, loss = 0.58919166\n","Iteration 35, loss = 0.58974482\n","Iteration 36, loss = 0.58910303\n","Iteration 37, loss = 0.58879440\n","Iteration 38, loss = 0.58866117\n","Iteration 39, loss = 0.58679310\n","Iteration 40, loss = 0.58791262\n","Iteration 41, loss = 0.58756640\n","Iteration 42, loss = 0.58789973\n","Iteration 43, loss = 0.58852523\n","Iteration 44, loss = 0.58829944\n","Iteration 45, loss = 0.58869669\n","Iteration 46, loss = 0.58880984\n","Iteration 47, loss = 0.58783594\n","Iteration 48, loss = 0.58774310\n","Iteration 49, loss = 0.58808025\n","Iteration 50, loss = 0.58869094\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.66407613\n","Iteration 2, loss = 0.64587657\n","Iteration 3, loss = 0.63688990\n","Iteration 4, loss = 0.63035022\n","Iteration 5, loss = 0.62445381\n","Iteration 6, loss = 0.61882182\n","Iteration 7, loss = 0.61393290\n","Iteration 8, loss = 0.60960048\n","Iteration 9, loss = 0.60534433\n","Iteration 10, loss = 0.60200973\n","Iteration 11, loss = 0.59938838\n","Iteration 12, loss = 0.59795097\n","Iteration 13, loss = 0.59620008\n","Iteration 14, loss = 0.59584416\n","Iteration 15, loss = 0.59532538\n","Iteration 16, loss = 0.59342864\n","Iteration 17, loss = 0.59332530\n","Iteration 18, loss = 0.59337913\n","Iteration 19, loss = 0.59259327\n","Iteration 20, loss = 0.59294097\n","Iteration 21, loss = 0.59249702\n","Iteration 22, loss = 0.59224123\n","Iteration 23, loss = 0.59214526\n","Iteration 24, loss = 0.59200974\n","Iteration 25, loss = 0.59332359\n","Iteration 26, loss = 0.59142486\n","Iteration 27, loss = 0.59100204\n","Iteration 28, loss = 0.59179658\n","Iteration 29, loss = 0.59109332\n","Iteration 30, loss = 0.59161387\n","Iteration 31, loss = 0.59144356\n","Iteration 32, loss = 0.59097125\n","Iteration 33, loss = 0.59068139\n","Iteration 34, loss = 0.59101123\n","Iteration 35, loss = 0.59064597\n","Iteration 36, loss = 0.59083803\n","Iteration 37, loss = 0.59076611\n","Iteration 38, loss = 0.58963691\n","Iteration 39, loss = 0.59125916\n","Iteration 40, loss = 0.58926815\n","Iteration 41, loss = 0.59069789\n","Iteration 42, loss = 0.58971938\n","Iteration 43, loss = 0.59027814\n","Iteration 44, loss = 0.58946473\n","Iteration 45, loss = 0.58982922\n","Iteration 46, loss = 0.58967079\n","Iteration 47, loss = 0.58955910\n","Iteration 48, loss = 0.58991778\n","Iteration 49, loss = 0.58937880\n","Iteration 50, loss = 0.58903779\n","Iteration 51, loss = 0.58908753\n","Iteration 52, loss = 0.58943620\n","Iteration 53, loss = 0.58939000\n","Iteration 54, loss = 0.58939339\n","Iteration 55, loss = 0.58951158\n","Iteration 56, loss = 0.58899884\n","Iteration 57, loss = 0.58887829\n","Iteration 58, loss = 0.58879711\n","Iteration 59, loss = 0.58871781\n","Iteration 60, loss = 0.58923119\n","Iteration 61, loss = 0.58950665\n","Iteration 62, loss = 0.58849240\n","Iteration 63, loss = 0.58817145\n","Iteration 64, loss = 0.58843989\n","Iteration 65, loss = 0.58820033\n","Iteration 66, loss = 0.58719655\n","Iteration 67, loss = 0.58756440\n","Iteration 68, loss = 0.58876872\n","Iteration 69, loss = 0.58833962\n","Iteration 70, loss = 0.58845472\n","Iteration 71, loss = 0.58786230\n","Iteration 72, loss = 0.58846571\n","Iteration 73, loss = 0.58792156\n","Iteration 74, loss = 0.58840473\n","Iteration 75, loss = 0.58768490\n","Iteration 76, loss = 0.58782158\n","Iteration 77, loss = 0.58776431\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.69164015\n","Iteration 2, loss = 0.64726329\n","Iteration 3, loss = 0.63413324\n","Iteration 4, loss = 0.62704443\n","Iteration 5, loss = 0.62197780\n","Iteration 6, loss = 0.61853208\n","Iteration 7, loss = 0.61394891\n","Iteration 8, loss = 0.61075731\n","Iteration 9, loss = 0.60833950\n","Iteration 10, loss = 0.60465069\n","Iteration 11, loss = 0.60175845\n","Iteration 12, loss = 0.60022492\n","Iteration 13, loss = 0.59796263\n","Iteration 14, loss = 0.59602510\n","Iteration 15, loss = 0.59459672\n","Iteration 16, loss = 0.59403307\n","Iteration 17, loss = 0.59313263\n","Iteration 18, loss = 0.59313792\n","Iteration 19, loss = 0.59293381\n","Iteration 20, loss = 0.59064249\n","Iteration 21, loss = 0.59159948\n","Iteration 22, loss = 0.59046685\n","Iteration 23, loss = 0.59092363\n","Iteration 24, loss = 0.59232333\n","Iteration 25, loss = 0.58945688\n","Iteration 26, loss = 0.58945411\n","Iteration 27, loss = 0.58873761\n","Iteration 28, loss = 0.58938461\n","Iteration 29, loss = 0.58975731\n","Iteration 30, loss = 0.58927508\n","Iteration 31, loss = 0.58818336\n","Iteration 32, loss = 0.58873402\n","Iteration 33, loss = 0.59070727\n","Iteration 34, loss = 0.58879710\n","Iteration 35, loss = 0.58640311\n","Iteration 36, loss = 0.58746228\n","Iteration 37, loss = 0.58702839\n","Iteration 38, loss = 0.58730376\n","Iteration 39, loss = 0.58771578\n","Iteration 40, loss = 0.58690440\n","Iteration 41, loss = 0.58725159\n","Iteration 42, loss = 0.58736234\n","Iteration 43, loss = 0.58680964\n","Iteration 44, loss = 0.58709641\n","Iteration 45, loss = 0.58665936\n","Iteration 46, loss = 0.58642978\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.66987393\n","Iteration 2, loss = 0.63694522\n","Iteration 3, loss = 0.62320226\n","Iteration 4, loss = 0.61413448\n","Iteration 5, loss = 0.60695841\n","Iteration 6, loss = 0.60270414\n","Iteration 7, loss = 0.59867604\n","Iteration 8, loss = 0.59431085\n","Iteration 9, loss = 0.59279422\n","Iteration 10, loss = 0.59144326\n","Iteration 11, loss = 0.58961318\n","Iteration 12, loss = 0.58855466\n","Iteration 13, loss = 0.58773401\n","Iteration 14, loss = 0.58805470\n","Iteration 15, loss = 0.58712702\n","Iteration 16, loss = 0.58765316\n","Iteration 17, loss = 0.58704528\n","Iteration 18, loss = 0.58616546\n","Iteration 19, loss = 0.58525731\n","Iteration 20, loss = 0.58645304\n","Iteration 21, loss = 0.58558365\n","Iteration 22, loss = 0.58540645\n","Iteration 23, loss = 0.58489074\n","Iteration 24, loss = 0.58329089\n","Iteration 25, loss = 0.58465138\n","Iteration 26, loss = 0.58377061\n","Iteration 27, loss = 0.58283438\n","Iteration 28, loss = 0.58365635\n","Iteration 29, loss = 0.58233001\n","Iteration 30, loss = 0.58230151\n","Iteration 31, loss = 0.58243333\n","Iteration 32, loss = 0.58195370\n","Iteration 33, loss = 0.58426676\n","Iteration 34, loss = 0.58159118\n","Iteration 35, loss = 0.58172119\n","Iteration 36, loss = 0.58127330\n","Iteration 37, loss = 0.58292624\n","Iteration 38, loss = 0.58194571\n","Iteration 39, loss = 0.58131637\n","Iteration 40, loss = 0.58245777\n","Iteration 41, loss = 0.58190494\n","Iteration 42, loss = 0.58291118\n","Iteration 43, loss = 0.58109402\n","Iteration 44, loss = 0.58218656\n","Iteration 45, loss = 0.58138750\n","Iteration 46, loss = 0.58157863\n","Iteration 47, loss = 0.57991350\n","Iteration 48, loss = 0.58023664\n","Iteration 49, loss = 0.58175258\n","Iteration 50, loss = 0.58064385\n","Iteration 51, loss = 0.58098452\n","Iteration 52, loss = 0.58043315\n","Iteration 53, loss = 0.58018013\n","Iteration 54, loss = 0.58065814\n","Iteration 55, loss = 0.57916078\n","Iteration 56, loss = 0.58143824\n","Iteration 57, loss = 0.58080830\n","Iteration 58, loss = 0.57929532\n","Iteration 59, loss = 0.57930206\n","Iteration 60, loss = 0.58182909\n","Iteration 61, loss = 0.57958682\n","Iteration 62, loss = 0.58050031\n","Iteration 63, loss = 0.57909529\n","Iteration 64, loss = 0.57904506\n","Iteration 65, loss = 0.58006085\n","Iteration 66, loss = 0.57969930\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","Iteration 1, loss = 0.65065139\n","Iteration 2, loss = 0.63233666\n","Iteration 3, loss = 0.62319928\n","Iteration 4, loss = 0.61470784\n","Iteration 5, loss = 0.60969537\n","Iteration 6, loss = 0.60565013\n","Iteration 7, loss = 0.60120895\n","Iteration 8, loss = 0.59803628\n","Iteration 9, loss = 0.59714144\n","Iteration 10, loss = 0.59773827\n","Iteration 11, loss = 0.59475465\n","Iteration 12, loss = 0.59416218\n","Iteration 13, loss = 0.59381362\n","Iteration 14, loss = 0.59344567\n","Iteration 15, loss = 0.59275161\n","Iteration 16, loss = 0.59241240\n","Iteration 17, loss = 0.59234734\n","Iteration 18, loss = 0.59222603\n","Iteration 19, loss = 0.59240280\n","Iteration 20, loss = 0.59322208\n","Iteration 21, loss = 0.59132538\n","Iteration 22, loss = 0.59173240\n","Iteration 23, loss = 0.59144953\n","Iteration 24, loss = 0.59083983\n","Iteration 25, loss = 0.59175825\n","Iteration 26, loss = 0.59111102\n","Iteration 27, loss = 0.59109374\n","Iteration 28, loss = 0.59040148\n","Iteration 29, loss = 0.59106670\n","Iteration 30, loss = 0.59036778\n","Iteration 31, loss = 0.58962177\n","Iteration 32, loss = 0.59093245\n","Iteration 33, loss = 0.59056156\n","Iteration 34, loss = 0.58967150\n","Iteration 35, loss = 0.58951256\n","Iteration 36, loss = 0.58963847\n","Iteration 37, loss = 0.58883012\n","Iteration 38, loss = 0.58910324\n","Iteration 39, loss = 0.58886558\n","Iteration 40, loss = 0.58937960\n","Iteration 41, loss = 0.59042989\n","Iteration 42, loss = 0.58955397\n","Iteration 43, loss = 0.59007398\n","Iteration 44, loss = 0.58898417\n","Iteration 45, loss = 0.58854749\n","Iteration 46, loss = 0.58884367\n","Iteration 47, loss = 0.58868972\n","Iteration 48, loss = 0.58899014\n","Iteration 49, loss = 0.58919195\n","Iteration 50, loss = 0.58809806\n","Iteration 51, loss = 0.58813244\n","Iteration 52, loss = 0.58763544\n","Iteration 53, loss = 0.58856882\n","Iteration 54, loss = 0.58881148\n","Iteration 55, loss = 0.58789942\n","Iteration 56, loss = 0.58782155\n","Iteration 57, loss = 0.58838764\n","Iteration 58, loss = 0.58910185\n","Iteration 59, loss = 0.58784592\n","Iteration 60, loss = 0.58710408\n","Iteration 61, loss = 0.58846958\n","Iteration 62, loss = 0.58913002\n","Iteration 63, loss = 0.58695581\n","Iteration 64, loss = 0.58708312\n","Iteration 65, loss = 0.58752791\n","Iteration 66, loss = 0.58713766\n","Iteration 67, loss = 0.58699171\n","Iteration 68, loss = 0.58669784\n","Iteration 69, loss = 0.58759465\n","Iteration 70, loss = 0.58602056\n","Iteration 71, loss = 0.58652270\n","Iteration 72, loss = 0.58610581\n","Iteration 73, loss = 0.58658956\n","Iteration 74, loss = 0.58636433\n","Iteration 75, loss = 0.58597586\n","Iteration 76, loss = 0.58634894\n","Iteration 77, loss = 0.58590998\n","Iteration 78, loss = 0.58524535\n","Iteration 79, loss = 0.58761170\n","Iteration 80, loss = 0.58724708\n","Iteration 81, loss = 0.58587219\n","Iteration 82, loss = 0.58540043\n","Iteration 83, loss = 0.58685624\n","Iteration 84, loss = 0.58519698\n","Iteration 85, loss = 0.58599927\n","Iteration 86, loss = 0.58583357\n","Iteration 87, loss = 0.58516779\n","Iteration 88, loss = 0.58739277\n","Iteration 89, loss = 0.58550212\n","Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n","평균정확률: 68.56727617104977 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UoXBtlEGIYIe"},"source":[""],"execution_count":null,"outputs":[]}]}